{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-title",
   "metadata": {},
   "source": [
    "# DBSCAN Clustering - Temporal Pipeline\n",
    "\n",
    "Full temporal pipeline matching the OPTICS approach: 13-feature multi-timeframe engineering,\n",
    "adaptive eps selection via k-distance heuristic at each timestamp, pair stability,\n",
    "formation/dissolution detection, permutation test, and artifact saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:21:12.484646Z",
     "iopub.status.busy": "2026-02-23T22:21:12.484565Z",
     "iopub.status.idle": "2026-02-23T22:21:14.624863Z",
     "shell.execute_reply": "2026-02-23T22:21:14.624349Z"
    }
   },
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "import itertools\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-data-title",
   "metadata": {},
   "source": [
    "# Data Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-data",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:21:14.629917Z",
     "iopub.status.busy": "2026-02-23T22:21:14.629475Z",
     "iopub.status.idle": "2026-02-23T22:21:16.885948Z",
     "shell.execute_reply": "2026-02-23T22:21:16.885201Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[                       0%                       ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**                     5%                       ]  2 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**                     5%                       ]  2 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*****                 10%                       ]  4 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[******                12%                       ]  5 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*******               15%                       ]  6 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[********              17%                       ]  7 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[********              17%                       ]  7 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[***********           22%                       ]  9 of 41 completed\r",
      "[***********           22%                       ]  9 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*************         27%                       ]  11 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*************         27%                       ]  11 of 41 completed\r",
      "[***************       32%                       ]  13 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[****************      34%                       ]  14 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[******************    37%                       ]  15 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*******************   39%                       ]  16 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[********************  41%                       ]  17 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[********************* 44%                       ]  18 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************46%                       ]  19 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************49%                       ]  20 of 41 completed\r",
      "[**********************49%                       ]  20 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************54%*                      ]  22 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************56%**                     ]  23 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************59%***                    ]  24 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************61%****                   ]  25 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************63%*****                  ]  26 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************66%*******                ]  27 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************68%********               ]  28 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************71%*********              ]  29 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************73%**********             ]  30 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************76%***********            ]  31 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************78%************           ]  32 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************80%*************          ]  33 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************83%***************        ]  34 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************83%***************        ]  34 of 41 completed\r",
      "[**********************88%*****************      ]  36 of 41 completed\r",
      "[**********************88%*****************      ]  36 of 41 completed\r",
      "[**********************88%*****************      ]  36 of 41 completed\r",
      "[**********************88%*****************      ]  36 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************98%********************** ]  40 of 41 completed\r",
      "[*********************100%***********************]  41 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "stocks = [\n",
    "    \"^GSPC\",\n",
    "    \"NVDA\", \"TSM\", \"AVGO\", \"AMD\", \"INTC\", \"MU\", \"TXN\", \"QCOM\", \"ADI\", \"MCHP\",\n",
    "    \"ASML\", \"AMAT\", \"LRCX\", \"KLAC\", \"TER\", \"ENTG\", \"NVMI\", \"TOELY\",\n",
    "    \"ON\", \"NXPI\", \"STM\", \"LSCC\", \"MPWR\", \"QRVO\", \"SWKS\", \"ALAB\", \"CRDO\",\n",
    "    \"ARM\", \"SNPS\", \"CDNS\", \"CEVA\",\n",
    "    \"WDC\", \"STX\",\n",
    "    \"GFS\", \"MRVL\", \"MTSI\", \"POWI\", \"SMTC\", \"VICR\", \"CAMT\"\n",
    "]\n",
    "\n",
    "def fetch_data(stocks):\n",
    "    data = yf.download(tickers=stocks, period=\"252d\", interval=\"1h\",\n",
    "                       group_by='ticker', auto_adjust=True, threads=True)\n",
    "    price_series_list = []\n",
    "    for s in stocks:\n",
    "        try:\n",
    "            if s in data:\n",
    "                series = data[s]['Close']\n",
    "                series.name = s\n",
    "                price_series_list.append(series)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if price_series_list:\n",
    "        df = pd.concat(price_series_list, axis=1)\n",
    "        df = df.groupby(df.index.date).apply(lambda g: g.ffill()).droplevel(0)\n",
    "        return df\n",
    "    return pd.DataFrame()\n",
    "\n",
    "df = fetch_data(stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-feat-title",
   "metadata": {},
   "source": [
    "## Feature Engineering (Multi-Timeframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-features",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:21:16.890037Z",
     "iopub.status.busy": "2026-02-23T22:21:16.889839Z",
     "iopub.status.idle": "2026-02-23T22:21:17.468413Z",
     "shell.execute_reply": "2026-02-23T22:21:17.467337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING - MULTI-TIMEFRAME APPROACH\n",
      "================================================================================\n",
      "Short-term window: 50 hours (~1 week)\n",
      "Medium-term window: 147 hours (~3 weeks)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total rows: 63,152\n",
      "Rows dropped (NaN): 6,688 (9.6%)\n",
      "Date range: 2025-03-24 15:30:00+00:00 to 2026-02-23 20:30:00+00:00\n",
      "Unique tickers: 40\n",
      "Feature columns: 11\n",
      "\n",
      "FEATURE ENGINEERING COMPLETE\n"
     ]
    }
   ],
   "source": [
    "if isinstance(df.columns, pd.MultiIndex):\n",
    "    if 'Close' in df.columns.get_level_values(0):\n",
    "        df = df['Close']\n",
    "    elif 'Close' in df.columns.get_level_values(1):\n",
    "        df = df.xs('Close', axis=1, level=1)\n",
    "\n",
    "returns_df = df.pct_change().dropna()\n",
    "market_returns = returns_df['^GSPC']\n",
    "\n",
    "window_short = 50\n",
    "window_medium = 147\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING - MULTI-TIMEFRAME APPROACH\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Short-term window: {window_short} hours (~1 week)\")\n",
    "print(f\"Medium-term window: {window_medium} hours (~3 weeks)\")\n",
    "\n",
    "# SHORT-TERM FEATURES\n",
    "rolling_vol_short = returns_df.rolling(window=window_short).std()\n",
    "\n",
    "rolling_cov_mkt_short = returns_df.rolling(window=window_short).cov(market_returns)\n",
    "rolling_mkt_var_short = market_returns.rolling(window=window_short).var()\n",
    "rolling_beta_spx_short = rolling_cov_mkt_short.divide(rolling_mkt_var_short, axis=0)\n",
    "\n",
    "non_spx_returns = returns_df.drop(columns=['^GSPC'], errors='ignore').dropna(axis=1, how='all')\n",
    "sector_sum = non_spx_returns.sum(axis=1)\n",
    "n_stocks = non_spx_returns.count(axis=1)\n",
    "\n",
    "rolling_beta_sector_short = pd.DataFrame(index=returns_df.index, columns=non_spx_returns.columns)\n",
    "for ticker in non_spx_returns.columns:\n",
    "    loo_sector = (sector_sum - non_spx_returns[ticker].fillna(0)) / (n_stocks - 1).clip(lower=1)\n",
    "    cov = returns_df[ticker].rolling(window=window_short).cov(loo_sector)\n",
    "    var = loo_sector.rolling(window=window_short).var()\n",
    "    rolling_beta_sector_short[ticker] = cov / var\n",
    "\n",
    "# MEDIUM-TERM FEATURES\n",
    "rolling_vol_medium = returns_df.rolling(window=window_medium).std()\n",
    "\n",
    "rolling_cov_mkt_medium = returns_df.rolling(window=window_medium).cov(market_returns)\n",
    "rolling_mkt_var_medium = market_returns.rolling(window=window_medium).var()\n",
    "rolling_beta_spx_medium = rolling_cov_mkt_medium.divide(rolling_mkt_var_medium, axis=0)\n",
    "\n",
    "rolling_beta_sector_medium = pd.DataFrame(index=returns_df.index, columns=non_spx_returns.columns)\n",
    "for ticker in non_spx_returns.columns:\n",
    "    loo_sector = (sector_sum - non_spx_returns[ticker].fillna(0)) / (n_stocks - 1).clip(lower=1)\n",
    "    cov = returns_df[ticker].rolling(window=window_medium).cov(loo_sector)\n",
    "    var = loo_sector.rolling(window=window_medium).var()\n",
    "    rolling_beta_sector_medium[ticker] = cov / var\n",
    "\n",
    "# INSTANTANEOUS FEATURES\n",
    "def calculate_rsi(data, window=70):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "rsi_df = df.apply(calculate_rsi)\n",
    "momentum_5h = df.pct_change(periods=5)\n",
    "\n",
    "# REGIME CHANGE INDICATORS\n",
    "vol_regime_shift = (rolling_vol_short - rolling_vol_medium) / rolling_vol_medium\n",
    "beta_spx_regime_shift = rolling_beta_spx_short - rolling_beta_spx_medium\n",
    "beta_sector_regime_shift = rolling_beta_sector_short - rolling_beta_sector_medium\n",
    "\n",
    "# ASSEMBLE ts_df\n",
    "ts_data_list = []\n",
    "for ticker in stocks:\n",
    "    if ticker == '^GSPC' or ticker not in df.columns:\n",
    "        continue\n",
    "    temp_df = pd.DataFrame({\n",
    "        'Price': df[ticker],\n",
    "        'Returns': returns_df[ticker],\n",
    "        'Vol_Short': rolling_vol_short[ticker],\n",
    "        'Beta_SPX_Short': rolling_beta_spx_short[ticker],\n",
    "        'Beta_Sector_Short': rolling_beta_sector_short[ticker],\n",
    "        'Vol_Medium': rolling_vol_medium[ticker],\n",
    "        'Beta_SPX_Medium': rolling_beta_spx_medium[ticker],\n",
    "        'Beta_Sector_Medium': rolling_beta_sector_medium[ticker],\n",
    "        'RSI': rsi_df[ticker],\n",
    "        'Momentum_5H': momentum_5h[ticker],\n",
    "        'Vol_Regime_Shift': vol_regime_shift[ticker],\n",
    "        'Beta_SPX_Regime_Shift': beta_spx_regime_shift[ticker],\n",
    "        'Beta_Sector_Regime_Shift': beta_sector_regime_shift[ticker],\n",
    "    }, index=df.index)\n",
    "    temp_df['Ticker'] = ticker\n",
    "    ts_data_list.append(temp_df)\n",
    "\n",
    "if ts_data_list:\n",
    "    ts_df = pd.concat(ts_data_list).reset_index().set_index(['Datetime', 'Ticker'])\n",
    "    initial_rows = len(ts_df)\n",
    "    ts_df = ts_df.dropna()\n",
    "    dropped_rows = initial_rows - len(ts_df)\n",
    "\n",
    "    print(f\"\\nTotal rows: {len(ts_df):,}\")\n",
    "    print(f\"Rows dropped (NaN): {dropped_rows:,} ({dropped_rows/initial_rows:.1%})\")\n",
    "    print(f\"Date range: {ts_df.index.get_level_values('Datetime').min()} to {ts_df.index.get_level_values('Datetime').max()}\")\n",
    "    print(f\"Unique tickers: {ts_df.index.get_level_values('Ticker').nunique()}\")\n",
    "    print(f\"Feature columns: {len([c for c in ts_df.columns if c not in ['Price', 'Returns']])}\")\n",
    "\n",
    "print(\"\\nFEATURE ENGINEERING COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-clust-title",
   "metadata": {},
   "source": [
    "# DBSCAN Temporal Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-clustering",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:21:17.486596Z",
     "iopub.status.busy": "2026-02-23T22:21:17.481587Z",
     "iopub.status.idle": "2026-02-23T22:21:27.421779Z",
     "shell.execute_reply": "2026-02-23T22:21:27.420313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRANSIENT REGIME DETECTION - DBSCAN CLUSTERING\n",
      "================================================================================\n",
      "Data Density: 1579 valid hourly timestamps\n",
      "Date Range: 2025-03-24 15:30:00+00:00 to 2026-02-23 20:30:00+00:00\n",
      "\n",
      "Using features: ['Returns', 'Vol_Short', 'Beta_SPX_Short', 'Beta_Sector_Short', 'RSI', 'Momentum_5H', 'Vol_Regime_Shift', 'Beta_SPX_Regime_Shift', 'Beta_Sector_Regime_Shift']\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: Running DBSCAN Clustering (Hourly Snapshots)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/1579 timestamps... (100 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/1579 timestamps... (200 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/1579 timestamps... (300 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/1579 timestamps... (400 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/1579 timestamps... (500 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 600/1579 timestamps... (600 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 700/1579 timestamps... (700 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 800/1579 timestamps... (800 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 900/1579 timestamps... (900 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1000/1579 timestamps... (1000 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1100/1579 timestamps... (1100 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1200/1579 timestamps... (1200 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1300/1579 timestamps... (1300 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1400/1579 timestamps... (1400 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1500/1579 timestamps... (1500 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1 COMPLETE: Cluster Quality Summary\n",
      "================================================================================\n",
      "\n",
      "Timestamp Analysis:\n",
      "  Total timestamps processed: 1579\n",
      "  Valid clustering windows: 1579 (100.0%)\n",
      "  Invalid/skipped windows: 0 (0.0%)\n",
      "\n",
      "Valid Cluster Statistics:\n",
      "  Avg clusters per timestamp: 1.5\n",
      "  Avg noise percentage: 28.0%\n",
      "  Avg PCA variance retained: 92.9%\n",
      "\n",
      "Cluster History Generated:\n",
      "  Total rows: 63152\n",
      "  Unique timestamps: 1579\n"
     ]
    }
   ],
   "source": [
    "ts_df = ts_df[~ts_df.index.duplicated(keep='first')]\n",
    "\n",
    "density = ts_df.groupby(level='Datetime').size()\n",
    "valid_timestamps = density[density >= 5].index\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"TRANSIENT REGIME DETECTION - DBSCAN CLUSTERING\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Data Density: {len(valid_timestamps)} valid hourly timestamps\")\n",
    "print(f\"Date Range: {valid_timestamps.min()} to {valid_timestamps.max()}\")\n",
    "\n",
    "features_to_cluster = [\n",
    "    'Returns', 'Vol_Short', 'Beta_SPX_Short', 'Beta_Sector_Short',\n",
    "    'RSI', 'Momentum_5H', 'Vol_Regime_Shift',\n",
    "    'Beta_SPX_Regime_Shift', 'Beta_Sector_Regime_Shift'\n",
    "]\n",
    "print(f\"\\nUsing features: {features_to_cluster}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 1: Running DBSCAN Clustering (Hourly Snapshots)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "MIN_SAMPLES = 3\n",
    "\n",
    "cluster_results = []\n",
    "cluster_quality_log = []\n",
    "\n",
    "for i, ts in enumerate(valid_timestamps):\n",
    "    try:\n",
    "        snapshot = ts_df.xs(ts, level='Datetime')[features_to_cluster].dropna()\n",
    "        if len(snapshot) < 5:\n",
    "            continue\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(snapshot)\n",
    "\n",
    "        pca = PCA(n_components=0.90)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "        # Adaptive eps via k-distance heuristic\n",
    "        k = MIN_SAMPLES\n",
    "        nbrs = NearestNeighbors(n_neighbors=k + 1).fit(X_pca)\n",
    "        distances, _ = nbrs.kneighbors(X_pca)\n",
    "        k_distances = np.sort(distances[:, k])\n",
    "\n",
    "        # Try percentiles [50, 60, 70, 80] and pick best\n",
    "        best_eps = None\n",
    "        best_score = -np.inf\n",
    "        best_labels = None\n",
    "\n",
    "        for pct in [50, 60, 70, 80]:\n",
    "            eps_try = float(np.percentile(k_distances, pct))\n",
    "            if eps_try <= 0:\n",
    "                continue\n",
    "\n",
    "            db = DBSCAN(eps=eps_try, min_samples=MIN_SAMPLES, metric='euclidean')\n",
    "            lbls = db.fit_predict(X_pca)\n",
    "\n",
    "            n_clust = len(set(lbls)) - (1 if -1 in lbls else 0)\n",
    "            noise_frac = (lbls == -1).sum() / len(lbls)\n",
    "\n",
    "            # Score: weight cluster count heavily, penalize extreme noise\n",
    "            score = n_clust * 2.0\n",
    "            if noise_frac > 0.70:\n",
    "                score -= 5.0\n",
    "            elif noise_frac < 0.10:\n",
    "                score -= 2.0\n",
    "            elif 0.15 <= noise_frac <= 0.50:\n",
    "                score += 2.0\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_eps = eps_try\n",
    "                best_labels = lbls\n",
    "\n",
    "        if best_labels is None:\n",
    "            # Fallback: use 70th percentile\n",
    "            eps_fallback = float(np.percentile(k_distances, 70))\n",
    "            db = DBSCAN(eps=max(eps_fallback, 0.1), min_samples=MIN_SAMPLES, metric='euclidean')\n",
    "            best_labels = db.fit_predict(X_pca)\n",
    "\n",
    "        labels_final = best_labels\n",
    "        unique_clusters = len(set(labels_final)) - (1 if -1 in labels_final else 0)\n",
    "        noise_count = (labels_final == -1).sum()\n",
    "        noise_pct = noise_count / len(labels_final)\n",
    "        total_stocks = len(labels_final)\n",
    "\n",
    "        quality_metrics = {\n",
    "            'Datetime': ts,\n",
    "            'Total_Stocks': total_stocks,\n",
    "            'Unique_Clusters': unique_clusters,\n",
    "            'Noise_Count': noise_count,\n",
    "            'Noise_Pct': noise_pct,\n",
    "            'PCA_Components': X_pca.shape[1],\n",
    "            'Variance_Explained': pca.explained_variance_ratio_.sum(),\n",
    "        }\n",
    "\n",
    "        is_valid = True\n",
    "        skip_reason = None\n",
    "\n",
    "        if unique_clusters < 1:\n",
    "            is_valid = False\n",
    "            skip_reason = \"No clusters found (all noise)\"\n",
    "        elif noise_pct > 0.75:\n",
    "            is_valid = False\n",
    "            skip_reason = f\"Too noisy ({noise_pct:.1%} noise)\"\n",
    "        elif unique_clusters > total_stocks * 0.75:\n",
    "            is_valid = False\n",
    "            skip_reason = f\"Over-fragmented ({unique_clusters} clusters for {total_stocks} stocks)\"\n",
    "\n",
    "        quality_metrics['Is_Valid'] = is_valid\n",
    "        quality_metrics['Skip_Reason'] = skip_reason\n",
    "        cluster_quality_log.append(quality_metrics)\n",
    "\n",
    "        if is_valid:\n",
    "            snapshot['Cluster_ID'] = labels_final\n",
    "            snapshot['Datetime'] = ts\n",
    "            snapshot['Num_Clusters'] = unique_clusters\n",
    "            snapshot['Noise_Pct'] = noise_pct\n",
    "            cluster_results.append(snapshot.reset_index())\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            valid_so_far = len(cluster_results)\n",
    "            print(f\"  Processed {i+1}/{len(valid_timestamps)} timestamps... ({valid_so_far} valid so far)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        cluster_quality_log.append({\n",
    "            'Datetime': ts, 'Total_Stocks': np.nan, 'Unique_Clusters': np.nan,\n",
    "            'Noise_Count': np.nan, 'Noise_Pct': np.nan,\n",
    "            'PCA_Components': np.nan, 'Variance_Explained': np.nan,\n",
    "            'Is_Valid': False, 'Skip_Reason': f\"Error: {str(e)[:50]}\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "if not cluster_results:\n",
    "    raise ValueError(\"No valid clusters found.\")\n",
    "\n",
    "cluster_history = pd.concat(cluster_results, ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 1 COMPLETE: Cluster Quality Summary\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "df_quality = pd.DataFrame(cluster_quality_log)\n",
    "total_timestamps = len(df_quality)\n",
    "valid_timestamps_count = df_quality['Is_Valid'].sum()\n",
    "invalid_timestamps_count = total_timestamps - valid_timestamps_count\n",
    "\n",
    "print(f\"\\nTimestamp Analysis:\")\n",
    "print(f\"  Total timestamps processed: {total_timestamps}\")\n",
    "print(f\"  Valid clustering windows: {valid_timestamps_count} ({valid_timestamps_count/total_timestamps:.1%})\")\n",
    "print(f\"  Invalid/skipped windows: {invalid_timestamps_count} ({invalid_timestamps_count/total_timestamps:.1%})\")\n",
    "\n",
    "if invalid_timestamps_count > 0:\n",
    "    print(f\"\\nSkip Reasons:\")\n",
    "    skip_summary = df_quality[~df_quality['Is_Valid']]['Skip_Reason'].value_counts()\n",
    "    for reason, count in skip_summary.items():\n",
    "        print(f\"  - {reason}: {count}\")\n",
    "\n",
    "valid_quality = df_quality[df_quality['Is_Valid']]\n",
    "if len(valid_quality) > 0:\n",
    "    print(f\"\\nValid Cluster Statistics:\")\n",
    "    print(f\"  Avg clusters per timestamp: {valid_quality['Unique_Clusters'].mean():.1f}\")\n",
    "    print(f\"  Avg noise percentage: {valid_quality['Noise_Pct'].mean():.1%}\")\n",
    "    print(f\"  Avg PCA variance retained: {valid_quality['Variance_Explained'].mean():.1%}\")\n",
    "\n",
    "print(f\"\\nCluster History Generated:\")\n",
    "print(f\"  Total rows: {len(cluster_history)}\")\n",
    "print(f\"  Unique timestamps: {cluster_history['Datetime'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-stability-title",
   "metadata": {},
   "source": [
    "# Pair Stability Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-stability",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:21:27.424251Z",
     "iopub.status.busy": "2026-02-23T22:21:27.424086Z",
     "iopub.status.idle": "2026-02-23T22:21:28.832441Z",
     "shell.execute_reply": "2026-02-23T22:21:28.831742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 2: Analyzing Cluster Stability\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pair Clustering Analysis:\n",
      "  Total unique pairs observed: 780\n",
      "  Pairs clustered together >50% of time: 333\n",
      "  Pairs clustered together >30% of time: 637\n",
      "  Pairs clustered together <10% of time: 77\n",
      "\n",
      "================================================================================\n",
      "TOP 15 MOST FREQUENTLY CO-CLUSTERED PAIRS\n",
      "================================================================================\n",
      "     Pair  Co_Cluster_Count  Co_Cluster_Frequency\n",
      " ADI-NXPI              1417              0.897403\n",
      " NXPI-TSM              1358              0.860038\n",
      "LRCX-NXPI              1347              0.853072\n",
      " LRCX-TSM              1341              0.849272\n",
      "  ADI-TSM              1340              0.848638\n",
      "NXPI-SWKS              1336              0.846105\n",
      "KLAC-LRCX              1336              0.846105\n",
      "AMAT-LRCX              1328              0.841039\n",
      " ADI-LRCX              1324              0.838505\n",
      " ADI-SWKS              1307              0.827739\n",
      "QRVO-SWKS              1305              0.826472\n",
      " ADI-AMAT              1293              0.818873\n",
      "  ADI-TXN              1287              0.815073\n",
      "AMAT-NXPI              1283              0.812540\n",
      "KLAC-NXPI              1278              0.809373\n",
      "\n",
      "================================================================================\n",
      "TOP 15 MOST TRANSIENT PAIRS (Rare Co-Clustering)\n",
      "================================================================================\n",
      "      Pair  Co_Cluster_Count  Co_Cluster_Frequency\n",
      "  CRDO-WDC                81              0.051298\n",
      " ALAB-CRDO                81              0.051298\n",
      " CDNS-CRDO                81              0.051298\n",
      "CRDO-TOELY                81              0.051298\n",
      "  ARM-CRDO                82              0.051932\n",
      "   CRDO-ON                85              0.053832\n",
      "   CRDO-MU                86              0.054465\n",
      " CAMT-CRDO                86              0.054465\n",
      " ALAB-CAMT                88              0.055731\n",
      " ALAB-QRVO                88              0.055731\n",
      " CRDO-MPWR                89              0.056365\n",
      " ALAB-ENTG                90              0.056998\n",
      " CRDO-SMTC                90              0.056998\n",
      " CRDO-POWI                90              0.056998\n",
      " CRDO-NVMI                92              0.058265\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 2: Analyzing Cluster Stability\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "pair_co_cluster_freq = {}\n",
    "\n",
    "for ts in cluster_history['Datetime'].unique():\n",
    "    snapshot = cluster_history[cluster_history['Datetime'] == ts]\n",
    "    for cluster_id in snapshot['Cluster_ID'].unique():\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        members = snapshot[snapshot['Cluster_ID'] == cluster_id]['Ticker'].tolist()\n",
    "        for s1, s2 in itertools.combinations(sorted(members), 2):\n",
    "            pair_key = (s1, s2)\n",
    "            if pair_key not in pair_co_cluster_freq:\n",
    "                pair_co_cluster_freq[pair_key] = 0\n",
    "            pair_co_cluster_freq[pair_key] += 1\n",
    "\n",
    "total_valid_windows = cluster_history['Datetime'].nunique()\n",
    "\n",
    "pair_stability_data = []\n",
    "for pair, count in pair_co_cluster_freq.items():\n",
    "    frequency = count / total_valid_windows\n",
    "    pair_stability_data.append({\n",
    "        'Ticker_1': pair[0],\n",
    "        'Ticker_2': pair[1],\n",
    "        'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "        'Co_Cluster_Count': count,\n",
    "        'Co_Cluster_Frequency': frequency,\n",
    "        'Is_Stable': frequency > 0.25\n",
    "    })\n",
    "\n",
    "df_pair_stability = pd.DataFrame(pair_stability_data).sort_values('Co_Cluster_Frequency', ascending=False)\n",
    "\n",
    "print(f\"\\nPair Clustering Analysis:\")\n",
    "print(f\"  Total unique pairs observed: {len(df_pair_stability)}\")\n",
    "print(f\"  Pairs clustered together >50% of time: {(df_pair_stability['Co_Cluster_Frequency'] > 0.50).sum()}\")\n",
    "print(f\"  Pairs clustered together >30% of time: {(df_pair_stability['Co_Cluster_Frequency'] > 0.30).sum()}\")\n",
    "print(f\"  Pairs clustered together <10% of time: {(df_pair_stability['Co_Cluster_Frequency'] < 0.10).sum()}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TOP 15 MOST FREQUENTLY CO-CLUSTERED PAIRS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(df_pair_stability[['Pair', 'Co_Cluster_Count', 'Co_Cluster_Frequency']].head(15).to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TOP 15 MOST TRANSIENT PAIRS (Rare Co-Clustering)\")\n",
    "print(f\"{'='*80}\")\n",
    "transient_pairs = df_pair_stability[\n",
    "    (df_pair_stability['Co_Cluster_Frequency'] > 0.05) &\n",
    "    (df_pair_stability['Co_Cluster_Frequency'] < 0.20)\n",
    "].sort_values('Co_Cluster_Frequency', ascending=True)\n",
    "print(transient_pairs[['Pair', 'Co_Cluster_Count', 'Co_Cluster_Frequency']].head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-formation-title",
   "metadata": {},
   "source": [
    "# Formation & Dissolution Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-formations",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:21:28.834331Z",
     "iopub.status.busy": "2026-02-23T22:21:28.834168Z",
     "iopub.status.idle": "2026-02-23T22:21:30.730990Z",
     "shell.execute_reply": "2026-02-23T22:21:30.730323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 2B: CLUSTER FORMATION & DISSOLUTION EVENTS\n",
      "================================================================================\n",
      "Tracking 780 pairs across 1579 timestamps...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formation events detected: 27663\n",
      "Dissolution events detected: 27663\n",
      "Complete cluster episodes: 27663\n",
      "\n",
      "Duration Statistics (hours):\n",
      "  Mean:   26.4\n",
      "  Median: 9.0\n",
      "  Min:    1\n",
      "  Max:    963\n",
      "\n",
      "================================================================================\n",
      "PAIR CLASSIFICATION\n",
      "================================================================================\n",
      "  transient           : 474 pairs\n",
      "  stable_candidate    : 306 pairs\n",
      "  sporadic            : 0 pairs\n",
      "  unknown             : 0 pairs\n",
      "\n",
      "Actionable formations (duration >= 5h): 17507\n",
      "\n",
      "FORMATION/DISSOLUTION ANALYSIS COMPLETE\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*80}\")\n",
    "print(\"PHASE 2B: CLUSTER FORMATION & DISSOLUTION EVENTS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "all_pairs = list(pair_co_cluster_freq.keys())\n",
    "all_timestamps = sorted(cluster_history['Datetime'].unique())\n",
    "\n",
    "print(f\"Tracking {len(all_pairs)} pairs across {len(all_timestamps)} timestamps...\")\n",
    "\n",
    "pair_coclustering = {}\n",
    "for ts in all_timestamps:\n",
    "    snapshot = cluster_history[cluster_history['Datetime'] == ts]\n",
    "    coclustered_at_ts = set()\n",
    "    for cluster_id in snapshot['Cluster_ID'].unique():\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        members = sorted(snapshot[snapshot['Cluster_ID'] == cluster_id]['Ticker'].tolist())\n",
    "        for s1, s2 in itertools.combinations(members, 2):\n",
    "            coclustered_at_ts.add((s1, s2))\n",
    "    for pair in all_pairs:\n",
    "        if pair not in pair_coclustering:\n",
    "            pair_coclustering[pair] = []\n",
    "        pair_coclustering[pair].append(1 if pair in coclustered_at_ts else 0)\n",
    "\n",
    "MIN_GAP_HOURS = 5\n",
    "MIN_EPISODE_HOURS = 5\n",
    "\n",
    "formation_events = []\n",
    "dissolution_events = []\n",
    "cluster_durations = []\n",
    "\n",
    "for pair, series in pair_coclustering.items():\n",
    "    ts_series = list(zip(all_timestamps, series))\n",
    "    in_cluster = False\n",
    "    formation_ts = None\n",
    "    formation_idx = None\n",
    "    gap_count = MIN_GAP_HOURS\n",
    "\n",
    "    for i, (ts, val) in enumerate(ts_series):\n",
    "        if val == 1:\n",
    "            if not in_cluster and gap_count >= MIN_GAP_HOURS:\n",
    "                in_cluster = True\n",
    "                formation_ts = ts\n",
    "                formation_idx = i\n",
    "                formation_events.append({\n",
    "                    'Ticker_1': pair[0], 'Ticker_2': pair[1],\n",
    "                    'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "                    'Formation_Time': ts, 'Timestamp_Index': i\n",
    "                })\n",
    "            elif not in_cluster:\n",
    "                in_cluster = True\n",
    "                formation_ts = ts\n",
    "                formation_idx = i\n",
    "            gap_count = 0\n",
    "        else:\n",
    "            if in_cluster:\n",
    "                gap_count += 1\n",
    "                if gap_count >= MIN_GAP_HOURS:\n",
    "                    last_cocluster_idx = i - gap_count\n",
    "                    dissolution_ts = all_timestamps[min(last_cocluster_idx + 1, len(all_timestamps) - 1)]\n",
    "                    duration = max(1, last_cocluster_idx - formation_idx + 1) if formation_idx is not None else 1\n",
    "                    dissolution_events.append({\n",
    "                        'Ticker_1': pair[0], 'Ticker_2': pair[1],\n",
    "                        'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "                        'Dissolution_Time': dissolution_ts, 'Duration_Hours': duration\n",
    "                    })\n",
    "                    if formation_ts is not None:\n",
    "                        cluster_durations.append({\n",
    "                            'Ticker_1': pair[0], 'Ticker_2': pair[1],\n",
    "                            'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "                            'Formation_Time': formation_ts,\n",
    "                            'Dissolution_Time': dissolution_ts,\n",
    "                            'Duration_Hours': duration\n",
    "                        })\n",
    "                    in_cluster = False\n",
    "                    formation_ts = None\n",
    "                    formation_idx = None\n",
    "            else:\n",
    "                gap_count += 1\n",
    "\n",
    "    if in_cluster and formation_ts is not None:\n",
    "        last_cocluster_idx = len(ts_series) - 1\n",
    "        for j in range(len(ts_series) - 1, -1, -1):\n",
    "            if ts_series[j][1] == 1:\n",
    "                last_cocluster_idx = j\n",
    "                break\n",
    "        duration = max(1, last_cocluster_idx - formation_idx + 1) if formation_idx is not None else 1\n",
    "        dissolution_ts = all_timestamps[min(last_cocluster_idx + 1, len(all_timestamps) - 1)]\n",
    "        dissolution_events.append({\n",
    "            'Ticker_1': pair[0], 'Ticker_2': pair[1],\n",
    "            'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "            'Dissolution_Time': dissolution_ts, 'Duration_Hours': duration\n",
    "        })\n",
    "        cluster_durations.append({\n",
    "            'Ticker_1': pair[0], 'Ticker_2': pair[1],\n",
    "            'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "            'Formation_Time': formation_ts,\n",
    "            'Dissolution_Time': dissolution_ts,\n",
    "            'Duration_Hours': duration\n",
    "        })\n",
    "\n",
    "df_formations = pd.DataFrame(formation_events)\n",
    "df_dissolutions = pd.DataFrame(dissolution_events)\n",
    "df_durations = pd.DataFrame(cluster_durations)\n",
    "\n",
    "print(f\"\\nFormation events detected: {len(df_formations)}\")\n",
    "print(f\"Dissolution events detected: {len(df_dissolutions)}\")\n",
    "print(f\"Complete cluster episodes: {len(df_durations)}\")\n",
    "\n",
    "if len(df_durations) > 0:\n",
    "    print(f\"\\nDuration Statistics (hours):\")\n",
    "    print(f\"  Mean:   {df_durations['Duration_Hours'].mean():.1f}\")\n",
    "    print(f\"  Median: {df_durations['Duration_Hours'].median():.1f}\")\n",
    "    print(f\"  Min:    {df_durations['Duration_Hours'].min():.0f}\")\n",
    "    print(f\"  Max:    {df_durations['Duration_Hours'].max():.0f}\")\n",
    "\n",
    "# Pair classification\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PAIR CLASSIFICATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "pair_formation_counts = df_formations.groupby('Pair').size().reset_index(name='Formation_Count')\n",
    "pair_avg_duration = df_durations.groupby('Pair')['Duration_Hours'].mean().reset_index(name='Avg_Duration')\n",
    "\n",
    "pair_classification = pair_formation_counts.merge(pair_avg_duration, on='Pair', how='left')\n",
    "pair_classification = pair_classification.merge(\n",
    "    df_pair_stability[['Pair', 'Co_Cluster_Frequency']], on='Pair', how='left'\n",
    ")\n",
    "\n",
    "pair_classification['Category'] = 'unknown'\n",
    "transient_mask = (pair_classification['Formation_Count'] >= 3) & (pair_classification['Avg_Duration'] <= 30)\n",
    "pair_classification.loc[transient_mask, 'Category'] = 'transient'\n",
    "\n",
    "stable_mask = ((pair_classification['Co_Cluster_Frequency'] > 0.25) | (pair_classification['Avg_Duration'] > 100)) & (pair_classification['Category'] == 'unknown')\n",
    "pair_classification.loc[stable_mask, 'Category'] = 'stable_candidate'\n",
    "\n",
    "sporadic_mask = (pair_classification['Formation_Count'] <= 2) & (pair_classification['Category'] == 'unknown')\n",
    "pair_classification.loc[sporadic_mask, 'Category'] = 'sporadic'\n",
    "\n",
    "for cat in ['transient', 'stable_candidate', 'sporadic', 'unknown']:\n",
    "    count = len(pair_classification[pair_classification['Category'] == cat])\n",
    "    print(f\"  {cat:20s}: {count} pairs\")\n",
    "\n",
    "# Actionable formations\n",
    "if len(df_durations) > 0:\n",
    "    df_formations_actionable = df_formations.merge(\n",
    "        df_durations[['Pair', 'Formation_Time', 'Duration_Hours']],\n",
    "        on=['Pair', 'Formation_Time'], how='inner'\n",
    "    ).query('Duration_Hours >= @MIN_EPISODE_HOURS')\n",
    "    print(f\"\\nActionable formations (duration >= {MIN_EPISODE_HOURS}h): {len(df_formations_actionable)}\")\n",
    "else:\n",
    "    df_formations_actionable = df_formations.copy()\n",
    "\n",
    "print(f\"\\nFORMATION/DISSOLUTION ANALYSIS COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-perm-title",
   "metadata": {},
   "source": [
    "# Correctness Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-permutation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:21:30.733740Z",
     "iopub.status.busy": "2026-02-23T22:21:30.733536Z",
     "iopub.status.idle": "2026-02-23T22:21:47.270456Z",
     "shell.execute_reply": "2026-02-23T22:21:47.269207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CORRECTNESS CHECK 1: Feature-Shuffle Permutation Test (DBSCAN)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Permutation 10/30 complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Permutation 20/30 complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Permutation 30/30 complete\n",
      "\n",
      "Fraction of pairs significant at p<0.05: 29.7%\n",
      "\n",
      "Top 15 pairs by permutation Z-score:\n",
      "  NXPI  -SWKS    Z = 9.60\n",
      "  ADI   -AMAT    Z = 8.94\n",
      "  ADI   -NXPI    Z = 8.53\n",
      "  ADI   -TSM     Z = 8.41\n",
      "  KLAC  -NXPI    Z = 8.13\n",
      "  LRCX  -NXPI    Z = 8.11\n",
      "  NXPI  -STM     Z = 7.92\n",
      "  NXPI  -QRVO    Z = 7.90\n",
      "  ADI   -SWKS    Z = 7.83\n",
      "  NXPI  -TSM     Z = 7.14\n",
      "  NXPI  -TXN     Z = 7.03\n",
      "  AMAT  -TSM     Z = 7.01\n",
      "  AMAT  -KLAC    Z = 6.90\n",
      "  ADI   -ASML    Z = 6.60\n",
      "  AMAT  -NXPI    Z = 6.59\n",
      "\n",
      "================================================================================\n",
      "CORRECTNESS CHECK 2: Out-of-Sample Split\n",
      "================================================================================\n",
      "\n",
      "Split point: 2025-10-30 16:30:00+00:00\n",
      "Training period: 2025-03-24 15:30:00+00:00 to 2025-10-30 16:30:00+00:00 (42314 rows)\n",
      "Test period: 2025-10-30 16:30:00+00:00 to 2026-02-23 20:30:00+00:00 (20838 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairs observed in both periods: 780\n",
      "Correlation of co-clustering frequency (train vs test): 0.839 (p=0.0000)\n",
      "RESULT: Good out-of-sample stability\n",
      "\n",
      "================================================================================\n",
      "ALL CORRECTNESS CHECKS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'='*80}\")\n",
    "print(\"CORRECTNESS CHECK 1: Feature-Shuffle Permutation Test (DBSCAN)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "n_permutations = 30\n",
    "n_sample_timestamps = 80\n",
    "\n",
    "all_ts_index = ts_df.index.get_level_values('Datetime').unique()\n",
    "rng = np.random.default_rng(42)\n",
    "if len(all_ts_index) > n_sample_timestamps:\n",
    "    sample_ts = rng.choice(all_ts_index, size=n_sample_timestamps, replace=False)\n",
    "else:\n",
    "    sample_ts = all_ts_index\n",
    "\n",
    "scale = len(sample_ts) / total_valid_windows\n",
    "null_counts = {}\n",
    "\n",
    "for perm in range(n_permutations):\n",
    "    perm_counts = {}\n",
    "    for ts in sample_ts:\n",
    "        try:\n",
    "            snapshot = ts_df.xs(ts, level='Datetime')[features_to_cluster].dropna()\n",
    "        except KeyError:\n",
    "            continue\n",
    "        if len(snapshot) < 5:\n",
    "            continue\n",
    "\n",
    "        tickers = snapshot.index.tolist()\n",
    "        values = snapshot.values.copy()\n",
    "        np.random.shuffle(values)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(values)\n",
    "        pca_perm = PCA(n_components=0.90)\n",
    "        X_pca = pca_perm.fit_transform(X_scaled)\n",
    "\n",
    "        # Adaptive eps on shuffled data\n",
    "        k = MIN_SAMPLES\n",
    "        nbrs = NearestNeighbors(n_neighbors=k + 1).fit(X_pca)\n",
    "        distances, _ = nbrs.kneighbors(X_pca)\n",
    "        k_distances_perm = np.sort(distances[:, k])\n",
    "\n",
    "        best_eps_perm = None\n",
    "        best_score_perm = -np.inf\n",
    "        best_labels_perm = None\n",
    "\n",
    "        for pct in [50, 60, 70, 80]:\n",
    "            eps_try = float(np.percentile(k_distances_perm, pct))\n",
    "            if eps_try <= 0:\n",
    "                continue\n",
    "            db = DBSCAN(eps=eps_try, min_samples=MIN_SAMPLES, metric='euclidean')\n",
    "            lbls = db.fit_predict(X_pca)\n",
    "            n_clust = len(set(lbls)) - (1 if -1 in lbls else 0)\n",
    "            noise_frac = (lbls == -1).sum() / len(lbls)\n",
    "\n",
    "            # Score: weight cluster count heavily, penalize extreme noise\n",
    "            score = n_clust * 2.0\n",
    "            if noise_frac > 0.70:\n",
    "                score -= 5.0\n",
    "            elif noise_frac < 0.10:\n",
    "                score -= 2.0\n",
    "            elif 0.15 <= noise_frac <= 0.50:\n",
    "                score += 2.0\n",
    "\n",
    "            if score > best_score_perm:\n",
    "                best_score_perm = score\n",
    "                best_labels_perm = lbls\n",
    "\n",
    "        if best_labels_perm is None:\n",
    "            eps_fallback = float(np.percentile(k_distances_perm, 70))\n",
    "            db = DBSCAN(eps=max(eps_fallback, 0.1), min_samples=MIN_SAMPLES)\n",
    "            best_labels_perm = db.fit_predict(X_pca)\n",
    "\n",
    "        perm_labels = best_labels_perm\n",
    "\n",
    "        for cid in set(perm_labels):\n",
    "            if cid == -1:\n",
    "                continue\n",
    "            members = sorted([tickers[j] for j in range(len(tickers)) if perm_labels[j] == cid])\n",
    "            for s1, s2 in itertools.combinations(members, 2):\n",
    "                key = (s1, s2)\n",
    "                perm_counts[key] = perm_counts.get(key, 0) + 1\n",
    "\n",
    "    for key, cnt in perm_counts.items():\n",
    "        null_counts.setdefault(key, []).append(cnt)\n",
    "\n",
    "    if (perm + 1) % 10 == 0:\n",
    "        print(f\"  Permutation {perm+1}/{n_permutations} complete\")\n",
    "\n",
    "for key in null_counts:\n",
    "    while len(null_counts[key]) < n_permutations:\n",
    "        null_counts[key].append(0)\n",
    "\n",
    "pair_zscores = {}\n",
    "for pair, obs_count in pair_co_cluster_freq.items():\n",
    "    obs_scaled = obs_count * scale\n",
    "    null_vals = np.array(null_counts.get(pair, [0] * n_permutations), dtype=float)\n",
    "    null_mean = null_vals.mean()\n",
    "    null_std = null_vals.std()\n",
    "    if null_std > 0:\n",
    "        z = (obs_scaled - null_mean) / null_std\n",
    "    else:\n",
    "        z = 0.0 if obs_scaled <= null_mean else np.inf\n",
    "    pair_zscores[pair] = float(z)\n",
    "\n",
    "n_significant = sum(1 for z in pair_zscores.values() if z > 1.96)\n",
    "frac_sig = n_significant / len(pair_zscores) if pair_zscores else 0.0\n",
    "\n",
    "print(f\"\\nFraction of pairs significant at p<0.05: {frac_sig:.1%}\")\n",
    "\n",
    "top_z = sorted(pair_zscores.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "print(f\"\\nTop 15 pairs by permutation Z-score:\")\n",
    "for pair, z in top_z:\n",
    "    print(f\"  {pair[0]:6s}-{pair[1]:6s}  Z = {z:.2f}\")\n",
    "\n",
    "# OOS split\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CORRECTNESS CHECK 2: Out-of-Sample Split\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "all_dates = sorted(cluster_history['Datetime'].unique())\n",
    "split_point = int(len(all_dates) * 0.67)\n",
    "split_timestamp = all_dates[split_point]\n",
    "\n",
    "train_history = cluster_history[cluster_history['Datetime'] <= split_timestamp]\n",
    "test_history = cluster_history[cluster_history['Datetime'] > split_timestamp]\n",
    "\n",
    "print(f\"\\nSplit point: {split_timestamp}\")\n",
    "print(f\"Training period: {all_dates[0]} to {split_timestamp} ({len(train_history)} rows)\")\n",
    "print(f\"Test period: {split_timestamp} to {all_dates[-1]} ({len(test_history)} rows)\")\n",
    "\n",
    "def calc_pair_freq(history_df):\n",
    "    freq = {}\n",
    "    total = history_df['Datetime'].nunique()\n",
    "    for ts in history_df['Datetime'].unique():\n",
    "        snap = history_df[history_df['Datetime'] == ts]\n",
    "        for cid in snap['Cluster_ID'].unique():\n",
    "            if cid == -1:\n",
    "                continue\n",
    "            members = sorted(snap[snap['Cluster_ID'] == cid]['Ticker'].tolist())\n",
    "            for s1, s2 in itertools.combinations(members, 2):\n",
    "                freq[(s1, s2)] = freq.get((s1, s2), 0) + 1\n",
    "    return {k: v / total for k, v in freq.items()}\n",
    "\n",
    "train_freq = calc_pair_freq(train_history)\n",
    "test_freq = calc_pair_freq(test_history)\n",
    "\n",
    "common_pairs = set(train_freq.keys()) & set(test_freq.keys())\n",
    "if len(common_pairs) > 0:\n",
    "    train_vals = [train_freq[p] for p in common_pairs]\n",
    "    test_vals = [test_freq[p] for p in common_pairs]\n",
    "    from scipy.stats import pearsonr\n",
    "    oos_corr, oos_pval = pearsonr(train_vals, test_vals)\n",
    "    print(f\"\\nPairs observed in both periods: {len(common_pairs)}\")\n",
    "    print(f\"Correlation of co-clustering frequency (train vs test): {oos_corr:.3f} (p={oos_pval:.4f})\")\n",
    "    if oos_corr > 0.5:\n",
    "        print(\"RESULT: Good out-of-sample stability\")\n",
    "    elif oos_corr > 0.2:\n",
    "        print(\"RESULT: Moderate out-of-sample stability\")\n",
    "    else:\n",
    "        print(\"WARNING: Poor out-of-sample stability\")\n",
    "\n",
    "oos_split_timestamp = split_timestamp\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL CORRECTNESS CHECKS COMPLETE\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-save-title",
   "metadata": {},
   "source": [
    "# Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-save",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:21:47.275382Z",
     "iopub.status.busy": "2026-02-23T22:21:47.274925Z",
     "iopub.status.idle": "2026-02-23T22:21:47.335178Z",
     "shell.execute_reply": "2026-02-23T22:21:47.334140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dbscan_ts_df -> /Users/jack/Documents/code/pairs-test/research/data/dbscan_ts_df.pkl (63152 items)\n",
      "Saved dbscan_df_formations -> /Users/jack/Documents/code/pairs-test/research/data/dbscan_df_formations.pkl (27663 items)\n",
      "Saved dbscan_df_formations_actionable -> /Users/jack/Documents/code/pairs-test/research/data/dbscan_df_formations_actionable.pkl (17507 items)\n",
      "Saved dbscan_pair_classification -> /Users/jack/Documents/code/pairs-test/research/data/dbscan_pair_classification.pkl (780 items)\n",
      "Saved dbscan_cluster_history -> /Users/jack/Documents/code/pairs-test/research/data/dbscan_cluster_history.pkl (63152 items)\n",
      "Saved dbscan_df_durations -> /Users/jack/Documents/code/pairs-test/research/data/dbscan_df_durations.pkl (27663 items)\n",
      "Saved dbscan_df_pair_stability -> /Users/jack/Documents/code/pairs-test/research/data/dbscan_df_pair_stability.pkl (780 items)\n",
      "Saved dbscan_oos_split_timestamp -> /Users/jack/Documents/code/pairs-test/research/data/dbscan_oos_split_timestamp.pkl (1 items)\n",
      "Saved dbscan_pair_co_cluster_freq -> /Users/jack/Documents/code/pairs-test/research/data/dbscan_pair_co_cluster_freq.pkl (780 items)\n",
      "\n",
      "All DBSCAN artifacts saved to /Users/jack/Documents/code/pairs-test/research/data/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "data_dir = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "save_items = {\n",
    "    'dbscan_ts_df': ts_df,\n",
    "    'dbscan_df_formations': df_formations,\n",
    "    'dbscan_df_formations_actionable': df_formations_actionable,\n",
    "    'dbscan_pair_classification': pair_classification,\n",
    "    'dbscan_cluster_history': cluster_history,\n",
    "    'dbscan_df_durations': df_durations,\n",
    "    'dbscan_df_pair_stability': df_pair_stability,\n",
    "    'dbscan_oos_split_timestamp': oos_split_timestamp,\n",
    "    'dbscan_pair_co_cluster_freq': pair_co_cluster_freq,\n",
    "}\n",
    "\n",
    "for name, obj in save_items.items():\n",
    "    path = os.path.join(data_dir, f'{name}.pkl')\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    size = len(obj) if hasattr(obj, '__len__') else 1\n",
    "    print(f'Saved {name} -> {path} ({size} items)')\n",
    "\n",
    "print(f'\\nAll DBSCAN artifacts saved to {data_dir}/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
