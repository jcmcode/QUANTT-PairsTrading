{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c551ab07",
   "metadata": {},
   "source": [
    "#   Optics Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa0dc1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:21:09.847743Z",
     "iopub.status.busy": "2026-02-23T22:21:09.847454Z",
     "iopub.status.idle": "2026-02-23T22:21:13.219864Z",
     "shell.execute_reply": "2026-02-23T22:21:13.218677Z"
    }
   },
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import os\n",
    "import sys\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import OPTICS\n",
    "from statsmodels.tsa.stattools import coint, grangercausalitytests\n",
    "import warnings\n",
    "import itertools\n",
    "import matplotlib.gridspec as gridspec\n",
    "import statsmodels.api as sm\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Add project root to path (handles both CWD=project_root and CWD=research/)\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from validation.pair_validation import feature_shuffle_permutation_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e12cae",
   "metadata": {},
   "source": [
    "# Data Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a64ad78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:21:13.222795Z",
     "iopub.status.busy": "2026-02-23T22:21:13.222465Z",
     "iopub.status.idle": "2026-02-23T22:21:15.495980Z",
     "shell.execute_reply": "2026-02-23T22:21:15.495375Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[                       0%                       ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**                     5%                       ]  2 of 41 completed\r",
      "[**                     5%                       ]  2 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*****                 10%                       ]  4 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[******                12%                       ]  5 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[******                12%                       ]  5 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[********              17%                       ]  7 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********            20%                       ]  8 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[***********           22%                       ]  9 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[************          24%                       ]  10 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*************         27%                       ]  11 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**************        29%                       ]  12 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**************        29%                       ]  12 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[****************      34%                       ]  14 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[******************    37%                       ]  15 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*******************   39%                       ]  16 of 41 completed\r",
      "[*******************   39%                       ]  16 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[********************* 44%                       ]  18 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[********************* 44%                       ]  18 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************49%                       ]  20 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************51%                       ]  21 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************54%*                      ]  22 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************56%**                     ]  23 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************59%***                    ]  24 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************61%****                   ]  25 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************63%*****                  ]  26 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************66%*******                ]  27 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************68%********               ]  28 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************71%*********              ]  29 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************71%*********              ]  29 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************76%***********            ]  31 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************78%************           ]  32 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************80%*************          ]  33 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************83%***************        ]  34 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************85%****************       ]  35 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************88%*****************      ]  36 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************90%******************     ]  37 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************93%********************   ]  38 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************95%*********************  ]  39 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[**********************98%********************** ]  40 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  41 of 41 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "stocks = [\n",
    "    # S&P for Beta\n",
    "    \"^GSPC\",\n",
    "    # Megacap Leaders & Generalists\n",
    "    \"NVDA\", \"TSM\", \"AVGO\", \"AMD\", \"INTC\", \"MU\", \"TXN\", \"QCOM\", \"ADI\", \"MCHP\",\n",
    "    \n",
    "    # Equipment & Manufacturing\n",
    "    \"ASML\", \"AMAT\", \"LRCX\", \"KLAC\", \"TER\", \"ENTG\", \"NVMI\", \"TOELY\",\n",
    "    \n",
    "    # Specialized\n",
    "    \"ON\", \"NXPI\", \"STM\", \"LSCC\", \"MPWR\", \"QRVO\", \"SWKS\", \"ALAB\", \"CRDO\",\n",
    "    \n",
    "    # Intellectual Property & Design Software\n",
    "    \"ARM\", \"SNPS\", \"CDNS\", \"CEVA\",\n",
    "    \n",
    "    # Memory & Storage\n",
    "    \"WDC\", \"STX\", # Removed extra \"MU\" here\n",
    "    \n",
    "    # Emerging & Mid-Cap\n",
    "    \"GFS\", \"MRVL\", \"MTSI\", \"POWI\", \"SMTC\", \"VICR\", \"CAMT\"\n",
    "]\n",
    "\n",
    "def fetch_data(stocks):\n",
    "    data = yf.download(tickers=stocks, period=\"252d\", interval=\"1h\", group_by='ticker', auto_adjust=True, threads=True)\n",
    "    \n",
    "    price_series_list = []\n",
    "    for s in stocks:\n",
    "        try: \n",
    "            if s in data:\n",
    "                series = data[s]['Close']\n",
    "                series.name = s\n",
    "                price_series_list.append(series)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    if price_series_list:\n",
    "        df = pd.concat(price_series_list, axis=1)\n",
    "        # Market-hours-aware fill: only forward-fill within the same trading day\n",
    "        # to avoid creating false correlations across overnight/weekend gaps\n",
    "        df = df.groupby(df.index.date).apply(lambda g: g.ffill()).droplevel(0)\n",
    "        return df\n",
    "    return pd.DataFrame()\n",
    "\n",
    "df = fetch_data(stocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae0f61",
   "metadata": {},
   "source": [
    "## Factor Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22b24f88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:21:15.498128Z",
     "iopub.status.busy": "2026-02-23T22:21:15.497990Z",
     "iopub.status.idle": "2026-02-23T22:21:15.723045Z",
     "shell.execute_reply": "2026-02-23T22:21:15.721336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING - MULTI-TIMEFRAME APPROACH\n",
      "================================================================================\n",
      "Short-term window: 50 hours (~1 week)\n",
      "Medium-term window: 147 hours (~3 weeks)\n",
      "Optimizing for transient events: 10-50 hour duration\n",
      "\n",
      "Calculating SHORT-TERM features (primary regime indicators)...\n",
      "Calculating MEDIUM-TERM features (context indicators)...\n",
      "Calculating INSTANTANEOUS features (momentum indicators)...\n",
      "Calculating REGIME CHANGE indicators...\n",
      "\n",
      "Assembling time-series dataframe...\n",
      "\n",
      "================================================================================\n",
      "TIME-SERIES DATAFRAME CREATED SUCCESSFULLY\n",
      "================================================================================\n",
      "Total rows: 63,143\n",
      "Rows dropped (NaN): 6,697 (9.6%)\n",
      "Date range: 2025-03-24 15:30:00+00:00 to 2026-02-23 20:30:00+00:00\n",
      "Unique tickers: 40\n",
      "\n",
      "Feature columns: 13\n",
      "\n",
      "Sample data:\n",
      "                                       Price   Returns  Vol_Short  \\\n",
      "Datetime                  Ticker                                    \n",
      "2025-03-24 15:30:00+00:00 NVDA    121.750000 -0.002376   0.010164   \n",
      "2025-03-24 16:30:00+00:00 NVDA    121.728798 -0.000174   0.010112   \n",
      "2025-03-24 17:30:00+00:00 NVDA    121.815002  0.000708   0.010080   \n",
      "2025-03-24 18:30:00+00:00 NVDA    121.794998 -0.000164   0.010074   \n",
      "2025-03-24 19:30:00+00:00 NVDA    121.230003 -0.004639   0.010068   \n",
      "\n",
      "                                  Beta_SPX_Short  Beta_Sector_Short  \\\n",
      "Datetime                  Ticker                                      \n",
      "2025-03-24 15:30:00+00:00 NVDA          1.939146           1.068880   \n",
      "2025-03-24 16:30:00+00:00 NVDA          2.006156           1.061855   \n",
      "2025-03-24 17:30:00+00:00 NVDA          2.038850           1.074187   \n",
      "2025-03-24 18:30:00+00:00 NVDA          2.049470           1.074283   \n",
      "2025-03-24 19:30:00+00:00 NVDA          2.054149           1.106169   \n",
      "\n",
      "                                  Vol_Medium  Beta_SPX_Medium  \\\n",
      "Datetime                  Ticker                                \n",
      "2025-03-24 15:30:00+00:00 NVDA      0.014089         2.241407   \n",
      "2025-03-24 16:30:00+00:00 NVDA      0.014089         2.241531   \n",
      "2025-03-24 17:30:00+00:00 NVDA      0.014089         2.242427   \n",
      "2025-03-24 18:30:00+00:00 NVDA      0.014053         2.236864   \n",
      "2025-03-24 19:30:00+00:00 NVDA      0.014021         2.231201   \n",
      "\n",
      "                                  Beta_Sector_Medium        RSI  Momentum_5H  \\\n",
      "Datetime                  Ticker                                               \n",
      "2025-03-24 15:30:00+00:00 NVDA              1.205974  60.375920     0.038901   \n",
      "2025-03-24 16:30:00+00:00 NVDA              1.208024  61.310433     0.042332   \n",
      "2025-03-24 17:30:00+00:00 NVDA              1.207464  61.646708     0.035534   \n",
      "2025-03-24 18:30:00+00:00 NVDA              1.205759  61.732762     0.010915   \n",
      "2025-03-24 19:30:00+00:00 NVDA              1.204493  60.975215    -0.006637   \n",
      "\n",
      "                                  Momentum_10H  Momentum_Accel  \\\n",
      "Datetime                  Ticker                                 \n",
      "2025-03-24 15:30:00+00:00 NVDA        0.027686        0.049697   \n",
      "2025-03-24 16:30:00+00:00 NVDA        0.045157        0.039623   \n",
      "2025-03-24 17:30:00+00:00 NVDA        0.047511        0.023968   \n",
      "2025-03-24 18:30:00+00:00 NVDA        0.037878       -0.015758   \n",
      "2025-03-24 19:30:00+00:00 NVDA        0.035579       -0.049135   \n",
      "\n",
      "                                  Vol_Regime_Shift  Beta_SPX_Regime_Shift  \\\n",
      "Datetime                  Ticker                                            \n",
      "2025-03-24 15:30:00+00:00 NVDA           -0.278600              -0.302261   \n",
      "2025-03-24 16:30:00+00:00 NVDA           -0.282260              -0.235375   \n",
      "2025-03-24 17:30:00+00:00 NVDA           -0.284538              -0.203576   \n",
      "2025-03-24 18:30:00+00:00 NVDA           -0.283124              -0.187393   \n",
      "2025-03-24 19:30:00+00:00 NVDA           -0.281881              -0.177052   \n",
      "\n",
      "                                  Beta_Sector_Regime_Shift  \n",
      "Datetime                  Ticker                            \n",
      "2025-03-24 15:30:00+00:00 NVDA                   -0.137094  \n",
      "2025-03-24 16:30:00+00:00 NVDA                   -0.146168  \n",
      "2025-03-24 17:30:00+00:00 NVDA                   -0.133277  \n",
      "2025-03-24 18:30:00+00:00 NVDA                   -0.131476  \n",
      "2025-03-24 19:30:00+00:00 NVDA                   -0.098324  \n",
      "\n",
      "================================================================================\n",
      "SKIPPING STATIC FUNDAMENTALS (Not relevant for transient detection)\n",
      "================================================================================\n",
      "Transient coupling is driven by events/market dynamics, not fundamental profiles.\n",
      "If you want to filter pairs by fundamentals later, re-enable this section.\n",
      "\n",
      "================================================================================\n",
      "FEATURE ENGINEERING COMPLETE - Ready for clustering\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FEATURE ENGINEERING FOR TRANSIENT REGIME DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Clean and Prepare Price Data\n",
    "if isinstance(df.columns, pd.MultiIndex):\n",
    "    if 'Close' in df.columns.get_level_values(0):\n",
    "        df = df['Close']\n",
    "    elif 'Close' in df.columns.get_level_values(1):\n",
    "        df = df.xs('Close', axis=1, level=1)\n",
    "\n",
    "# 2. Base Calculations\n",
    "returns_df = df.pct_change().dropna()\n",
    "market_returns = returns_df['^GSPC']\n",
    "\n",
    "# ============================================================================\n",
    "# CRITICAL CHANGE: Multi-Timeframe Feature Engineering\n",
    "# ============================================================================\n",
    "\n",
    "# SHORT-TERM WINDOW (Transient regime detection)\n",
    "window_short = 50  # ~1 week of hourly data - ALIGNED WITH TRADE DURATION\n",
    "\n",
    "# MEDIUM-TERM WINDOW (Context/stability check)\n",
    "window_medium = 147  # ~3 weeks - your original window\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING - MULTI-TIMEFRAME APPROACH\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Short-term window: {window_short} hours (~1 week)\")\n",
    "print(f\"Medium-term window: {window_medium} hours (~3 weeks)\")\n",
    "print(f\"Optimizing for transient events: 10-50 hour duration\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3A. SHORT-TERM FEATURES (Primary clustering features)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Calculating SHORT-TERM features (primary regime indicators)...\")\n",
    "\n",
    "# Feature A: SHORT-TERM Volatility (Recent risk behavior)\n",
    "rolling_vol_short = returns_df.rolling(window=window_short).std()\n",
    "\n",
    "# Feature B: SHORT-TERM Beta to SPX (Recent market sensitivity)\n",
    "rolling_cov_mkt_short = returns_df.rolling(window=window_short).cov(market_returns)\n",
    "rolling_mkt_var_short = market_returns.rolling(window=window_short).var()\n",
    "rolling_beta_spx_short = rolling_cov_mkt_short.divide(rolling_mkt_var_short, axis=0)\n",
    "\n",
    "# Feature C: SHORT-TERM Beta to Sector (Recent sector coupling)\n",
    "non_spx_returns = returns_df.drop(columns=['^GSPC'], errors='ignore').dropna(axis=1, how='all')\n",
    "sector_sum = non_spx_returns.sum(axis=1)\n",
    "n_stocks = non_spx_returns.count(axis=1)\n",
    "\n",
    "# Leave-one-out sector average and rolling beta for each ticker\n",
    "rolling_beta_sector_short = pd.DataFrame(index=returns_df.index, columns=non_spx_returns.columns)\n",
    "for ticker in non_spx_returns.columns:\n",
    "    loo_sector = (sector_sum - non_spx_returns[ticker].fillna(0)) / (n_stocks - 1).clip(lower=1)\n",
    "    cov = returns_df[ticker].rolling(window=window_short).cov(loo_sector)\n",
    "    var = loo_sector.rolling(window=window_short).var()\n",
    "    rolling_beta_sector_short[ticker] = cov / var\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3B. MEDIUM-TERM FEATURES (Context/stability indicators)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Calculating MEDIUM-TERM features (context indicators)...\")\n",
    "\n",
    "# These help identify if current behavior is unusual vs. longer-term baseline\n",
    "rolling_vol_medium = returns_df.rolling(window=window_medium).std()\n",
    "\n",
    "rolling_cov_mkt_medium = returns_df.rolling(window=window_medium).cov(market_returns)\n",
    "rolling_mkt_var_medium = market_returns.rolling(window=window_medium).var()\n",
    "rolling_beta_spx_medium = rolling_cov_mkt_medium.divide(rolling_mkt_var_medium, axis=0)\n",
    "\n",
    "rolling_beta_sector_medium = pd.DataFrame(index=returns_df.index, columns=non_spx_returns.columns)\n",
    "for ticker in non_spx_returns.columns:\n",
    "    loo_sector = (sector_sum - non_spx_returns[ticker].fillna(0)) / (n_stocks - 1).clip(lower=1)\n",
    "    cov = returns_df[ticker].rolling(window=window_medium).cov(loo_sector)\n",
    "    var = loo_sector.rolling(window=window_medium).var()\n",
    "    rolling_beta_sector_medium[ticker] = cov / var\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3C. INSTANTANEOUS FEATURES (Momentum/Overbought indicators)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Calculating INSTANTANEOUS features (momentum indicators)...\")\n",
    "\n",
    "# Feature D: RSI (Momentum/Overextended) - 70 periods for hourly data (~2 weeks)\n",
    "def calculate_rsi(data, window=70):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "rsi_df = df.apply(calculate_rsi)\n",
    "\n",
    "# Feature E: Short Term Momentum (5-period return)\n",
    "momentum_5h = df.pct_change(periods=5)\n",
    "\n",
    "# Feature F: Momentum Acceleration (change in momentum)\n",
    "momentum_10h = df.pct_change(periods=10)\n",
    "momentum_acceleration = momentum_5h - momentum_5h.shift(5)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3D. REGIME CHANGE INDICATORS (New!)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Calculating REGIME CHANGE indicators...\")\n",
    "\n",
    "# Detect when short-term behavior diverges from medium-term baseline\n",
    "# This helps identify when a NEW regime is forming\n",
    "\n",
    "# Volatility Regime Shift (is vol spiking vs. baseline?)\n",
    "vol_regime_shift = (rolling_vol_short - rolling_vol_medium) / rolling_vol_medium\n",
    "\n",
    "# Beta Regime Shift (is market sensitivity changing?)\n",
    "beta_spx_regime_shift = rolling_beta_spx_short - rolling_beta_spx_medium\n",
    "beta_sector_regime_shift = rolling_beta_sector_short - rolling_beta_sector_medium\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Assemble the Master Time-Series DataFrame (ts_df)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nAssembling time-series dataframe...\")\n",
    "\n",
    "ts_data_list = []\n",
    "\n",
    "for ticker in stocks:\n",
    "    if ticker == '^GSPC' or ticker not in df.columns: \n",
    "        continue\n",
    "    \n",
    "    # Extract features for this specific ticker\n",
    "    temp_df = pd.DataFrame({\n",
    "        # Price & Returns (baseline)\n",
    "        'Price': df[ticker],\n",
    "        'Returns': returns_df[ticker],\n",
    "        \n",
    "        # SHORT-TERM FEATURES (Primary clustering features)\n",
    "        'Vol_Short': rolling_vol_short[ticker],\n",
    "        'Beta_SPX_Short': rolling_beta_spx_short[ticker],\n",
    "        'Beta_Sector_Short': rolling_beta_sector_short[ticker],\n",
    "        \n",
    "        # MEDIUM-TERM FEATURES (Context)\n",
    "        'Vol_Medium': rolling_vol_medium[ticker],\n",
    "        'Beta_SPX_Medium': rolling_beta_spx_medium[ticker],\n",
    "        'Beta_Sector_Medium': rolling_beta_sector_medium[ticker],\n",
    "        \n",
    "        # INSTANTANEOUS FEATURES\n",
    "        'RSI': rsi_df[ticker],\n",
    "        'Momentum_5H': momentum_5h[ticker],\n",
    "        'Momentum_10H': momentum_10h[ticker],\n",
    "        'Momentum_Accel': momentum_acceleration[ticker],\n",
    "        \n",
    "        # REGIME CHANGE INDICATORS (New!)\n",
    "        'Vol_Regime_Shift': vol_regime_shift[ticker],\n",
    "        'Beta_SPX_Regime_Shift': beta_spx_regime_shift[ticker],\n",
    "        'Beta_Sector_Regime_Shift': beta_sector_regime_shift[ticker],\n",
    "        \n",
    "    }, index=df.index)\n",
    "    \n",
    "    temp_df['Ticker'] = ticker\n",
    "    ts_data_list.append(temp_df)\n",
    "\n",
    "if ts_data_list:\n",
    "    ts_df = pd.concat(ts_data_list).reset_index().set_index(['Datetime', 'Ticker'])\n",
    "    \n",
    "    # Drop NaNs created by rolling windows\n",
    "    initial_rows = len(ts_df)\n",
    "    ts_df = ts_df.dropna()\n",
    "    dropped_rows = initial_rows - len(ts_df)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"TIME-SERIES DATAFRAME CREATED SUCCESSFULLY\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total rows: {len(ts_df):,}\")\n",
    "    print(f\"Rows dropped (NaN): {dropped_rows:,} ({dropped_rows/initial_rows:.1%})\")\n",
    "    print(f\"Date range: {ts_df.index.get_level_values('Datetime').min()} to {ts_df.index.get_level_values('Datetime').max()}\")\n",
    "    print(f\"Unique tickers: {ts_df.index.get_level_values('Ticker').nunique()}\")\n",
    "    print(f\"\\nFeature columns: {len([c for c in ts_df.columns if c not in ['Price', 'Returns', 'Ticker']])}\")\n",
    "    print(\"\\nSample data:\")\n",
    "    print(ts_df.head())\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. OPTIONAL: Static Fundamental DataFrame (Keep or Remove?)\n",
    "# ============================================================================\n",
    "\n",
    "# NOTE: For transient regime detection, fundamentals are less relevant\n",
    "# Transient coupling is driven by events/news, not fundamental similarity\n",
    "# Consider REMOVING this section unless you plan to use it for filtering\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SKIPPING STATIC FUNDAMENTALS (Not relevant for transient detection)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"Transient coupling is driven by events/market dynamics, not fundamental profiles.\")\n",
    "print(\"If you want to filter pairs by fundamentals later, re-enable this section.\\n\")\n",
    "\n",
    "# Uncomment below if you want to keep fundamentals\n",
    "\"\"\"\n",
    "fundamental_list = []\n",
    "print(\"Fetching Static Fundamentals...\")\n",
    "\n",
    "for ticker in stocks:\n",
    "    if ticker == '^GSPC': continue\n",
    "    try:\n",
    "        t = yf.Ticker(ticker)\n",
    "        info = t.info\n",
    "        \n",
    "        fundamental_list.append({\n",
    "            'Ticker': ticker,\n",
    "            'Sector': info.get('sector', 'Unknown'),\n",
    "            'Industry': info.get('industry', 'Unknown'),\n",
    "            'Market_Cap': info.get('marketCap', np.nan),\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Could not fetch data for {ticker}: {e}\")\n",
    "        continue\n",
    "\n",
    "static_df = pd.DataFrame(fundamental_list).set_index('Ticker')\n",
    "print(\"Static DataFrame (static_df) Created Successfully!\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING COMPLETE - Ready for clustering\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf953a66",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "## Based on an Hourly Time Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ab02dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:21:15.725371Z",
     "iopub.status.busy": "2026-02-23T22:21:15.725231Z",
     "iopub.status.idle": "2026-02-23T22:21:56.486482Z",
     "shell.execute_reply": "2026-02-23T22:21:56.485408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRANSIENT REGIME DETECTION - OPTICS CLUSTERING\n",
      "================================================================================\n",
      "Data Density: 1579 valid hourly timestamps\n",
      "Date Range: 2025-03-24 15:30:00+00:00 to 2026-02-23 20:30:00+00:00\n",
      "\n",
      "Using features: ['Returns', 'Vol_Short', 'Beta_SPX_Short', 'Beta_Sector_Short', 'RSI', 'Momentum_5H', 'Vol_Regime_Shift', 'Beta_SPX_Regime_Shift', 'Beta_Sector_Regime_Shift']\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: Running OPTICS Clustering (Hourly Snapshots)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 100/1579 timestamps... (91 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 200/1579 timestamps... (179 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 300/1579 timestamps... (260 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 400/1579 timestamps... (338 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 500/1579 timestamps... (436 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 600/1579 timestamps... (521 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 700/1579 timestamps... (607 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 800/1579 timestamps... (701 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 900/1579 timestamps... (789 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1000/1579 timestamps... (865 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1100/1579 timestamps... (948 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1200/1579 timestamps... (1042 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1300/1579 timestamps... (1122 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1400/1579 timestamps... (1214 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processed 1500/1579 timestamps... (1307 valid so far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PHASE 1 COMPLETE: Cluster Quality Summary\n",
      "================================================================================\n",
      "\n",
      "Timestamp Analysis:\n",
      "  Total timestamps processed: 1579\n",
      "  Valid clustering windows: 1364 (86.4%)\n",
      "  Invalid/skipped windows: 215 (13.6%)\n",
      "\n",
      "Skip Reasons:\n",
      "  - Too noisy (77.5% noise): 65 (30.2%)\n",
      "  - Too noisy (80.0% noise): 52 (24.2%)\n",
      "  - Too noisy (82.5% noise): 45 (20.9%)\n",
      "  - Too noisy (85.0% noise): 23 (10.7%)\n",
      "  - Too noisy (90.0% noise): 11 (5.1%)\n",
      "  - Too noisy (92.5% noise): 10 (4.7%)\n",
      "  - Too noisy (87.5% noise): 5 (2.3%)\n",
      "  - Too noisy (76.9% noise): 3 (1.4%)\n",
      "  - Too noisy (84.6% noise): 1 (0.5%)\n",
      "\n",
      "Valid Cluster Statistics:\n",
      "  Avg clusters per timestamp: 3.4\n",
      "  Avg noise percentage: 58.5%\n",
      "  Avg PCA variance retained: 92.8%\n",
      "\n",
      "Cluster History Generated:\n",
      "  Total rows: 54547\n",
      "  Unique timestamps: 1364\n",
      "  Date range: 2025-03-24 16:30:00+00:00 to 2026-02-23 20:30:00+00:00\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: Analyzing Cluster Stability\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pair Clustering Analysis:\n",
      "  Total unique pairs observed: 776\n",
      "  Pairs clustered together >50% of time: 0\n",
      "  Pairs clustered together >30% of time: 4\n",
      "  Pairs clustered together <10% of time: 685\n",
      "\n",
      "================================================================================\n",
      "TOP 15 MOST FREQUENTLY CO-CLUSTERED PAIRS\n",
      "================================================================================\n",
      "     Pair  Co_Cluster_Count  Co_Cluster_Frequency\n",
      "QRVO-SWKS               522              0.382698\n",
      "KLAC-LRCX               497              0.364370\n",
      "AMAT-LRCX               493              0.361437\n",
      "AMAT-KLAC               445              0.326246\n",
      " ADI-NXPI               357              0.261730\n",
      "  ADI-TXN               350              0.256598\n",
      "MCHP-NXPI               317              0.232405\n",
      "QCOM-QRVO               310              0.227273\n",
      " ADI-SWKS               303              0.222141\n",
      "QCOM-SWKS               292              0.214076\n",
      " NXPI-STM               283              0.207478\n",
      " NXPI-TXN               277              0.203079\n",
      "  MCHP-ON               275              0.201613\n",
      "CDNS-SNPS               267              0.195748\n",
      " ADI-QCOM               267              0.195748\n",
      "\n",
      "================================================================================\n",
      "TOP 15 MOST TRANSIENT PAIRS (Rare Co-Clustering)\n",
      "================================================================================\n",
      "     Pair  Co_Cluster_Count  Co_Cluster_Frequency\n",
      "CEVA-MCHP                69              0.050587\n",
      "CDNS-KLAC                69              0.050587\n",
      "LSCC-SWKS                69              0.050587\n",
      "  ENTG-MU                69              0.050587\n",
      "AMAT-CDNS                69              0.050587\n",
      " CAMT-TSM                69              0.050587\n",
      " CDNS-GFS                69              0.050587\n",
      " STX-SWKS                69              0.050587\n",
      "LSCC-MTSI                70              0.051320\n",
      "MTSI-QCOM                70              0.051320\n",
      " AMD-AVGO                70              0.051320\n",
      " KLAC-WDC                70              0.051320\n",
      "LRCX-SMTC                70              0.051320\n",
      " AMD-NVMI                70              0.051320\n",
      "MPWR-QCOM                70              0.051320\n",
      "\n",
      "================================================================================\n",
      "PHASE 3: Temporal Analysis - When Do Regimes Shift?\n",
      "================================================================================\n",
      "\n",
      "Daily Clustering Variability:\n",
      "  Days with high differentiation (>4 clusters): 120\n",
      "  Days with low differentiation (â‰¤2 clusters): 1\n",
      "\n",
      "Potential Regime Shift Days (unusual cluster patterns):\n",
      "  48 days detected\n",
      "\n",
      "Top 5 Most Unusual Days:\n",
      "  2025-06-18: 7 clusters (avg: 4.5)\n",
      "  2025-04-04: 6 clusters (avg: 4.5)\n",
      "  2025-04-08: 6 clusters (avg: 4.5)\n",
      "  2025-04-11: 6 clusters (avg: 4.5)\n",
      "  2025-04-30: 6 clusters (avg: 4.5)\n",
      "\n",
      "================================================================================\n",
      "CLUSTERING PHASE COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Data structures created:\n",
      "  - cluster_history: 54547 rows\n",
      "  - df_quality: 1579 rows\n",
      "  - df_pair_stability: 776 rows\n",
      "\n",
      "Ready for pair testing phase\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# OPTICS CLUSTERING FOR TRANSIENT REGIME DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "if 'ts_df' not in locals():\n",
    "    raise ValueError(\"Please run the Feature Engineering cell to create 'ts_df' first.\")\n",
    "\n",
    "# Clean duplicates\n",
    "ts_df = ts_df[~ts_df.index.duplicated(keep='first')]\n",
    "\n",
    "# Check Density\n",
    "density = ts_df.groupby(level='Datetime').size()\n",
    "valid_timestamps = density[density >= 5].index\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"TRANSIENT REGIME DETECTION - OPTICS CLUSTERING\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Data Density: {len(valid_timestamps)} valid hourly timestamps\")\n",
    "print(f\"Date Range: {valid_timestamps.min()} to {valid_timestamps.max()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURES: Short-term + regime shift indicators for transient detection\n",
    "# ============================================================================\n",
    "features_to_cluster = ['Returns', 'Vol_Short', 'Beta_SPX_Short', 'Beta_Sector_Short', 'RSI', 'Momentum_5H', 'Vol_Regime_Shift', 'Beta_SPX_Regime_Shift', 'Beta_Sector_Regime_Shift']\n",
    "print(f\"\\nUsing features: {features_to_cluster}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CLUSTERING LOOP - Hourly Regime Detection\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 1: Running OPTICS Clustering (Hourly Snapshots)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cluster_results = []\n",
    "cluster_quality_log = []\n",
    "\n",
    "for i, ts in enumerate(valid_timestamps):\n",
    "    try:\n",
    "        snapshot = ts_df.xs(ts, level='Datetime')[features_to_cluster].dropna()\n",
    "        if len(snapshot) < 5: \n",
    "            continue\n",
    "        \n",
    "        # Scale & PCA (Dimensionality Reduction)\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(snapshot)\n",
    "        \n",
    "        pca = PCA(n_components=0.90)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # OPTICS Clustering\n",
    "        optics = OPTICS(min_samples=3, metric='euclidean', xi=0.05, min_cluster_size=3)\n",
    "        optics.fit(X_pca)\n",
    "        \n",
    "        # ===== CLUSTER QUALITY VALIDATION (RELAXED) =====\n",
    "        unique_clusters = len(set(optics.labels_)) - (1 if -1 in optics.labels_ else 0)\n",
    "        noise_count = (optics.labels_ == -1).sum()\n",
    "        noise_pct = noise_count / len(optics.labels_)\n",
    "        total_stocks = len(optics.labels_)\n",
    "        \n",
    "        # Quality Metrics\n",
    "        quality_metrics = {\n",
    "            'Datetime': ts,\n",
    "            'Total_Stocks': total_stocks,\n",
    "            'Unique_Clusters': unique_clusters,\n",
    "            'Noise_Count': noise_count,\n",
    "            'Noise_Pct': noise_pct,\n",
    "            'PCA_Components': X_pca.shape[1],\n",
    "            'Variance_Explained': pca.explained_variance_ratio_.sum()\n",
    "        }\n",
    "        \n",
    "        # RELAXED Quality Filters\n",
    "        is_valid = True\n",
    "        skip_reason = None\n",
    "        \n",
    "        if unique_clusters < 1:\n",
    "            is_valid = False\n",
    "            skip_reason = \"No clusters found (all noise)\"\n",
    "        elif noise_pct > 0.75:  # Relaxed to 75%\n",
    "            is_valid = False\n",
    "            skip_reason = f\"Too noisy ({noise_pct:.1%} noise)\"\n",
    "        elif unique_clusters > total_stocks * 0.75:\n",
    "            is_valid = False\n",
    "            skip_reason = f\"Over-fragmented ({unique_clusters} clusters for {total_stocks} stocks)\"\n",
    "        \n",
    "        quality_metrics['Is_Valid'] = is_valid\n",
    "        quality_metrics['Skip_Reason'] = skip_reason\n",
    "        cluster_quality_log.append(quality_metrics)\n",
    "        \n",
    "        # Store valid clusters\n",
    "        if is_valid:\n",
    "            snapshot['Cluster_ID'] = optics.labels_\n",
    "            snapshot['Datetime'] = ts\n",
    "            snapshot['Num_Clusters'] = unique_clusters\n",
    "            snapshot['Noise_Pct'] = noise_pct\n",
    "            cluster_results.append(snapshot.reset_index())\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 100 == 0:\n",
    "            valid_so_far = len(cluster_results)\n",
    "            print(f\"  Processed {i+1}/{len(valid_timestamps)} timestamps... ({valid_so_far} valid so far)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        cluster_quality_log.append({\n",
    "            'Datetime': ts,\n",
    "            'Total_Stocks': np.nan,\n",
    "            'Unique_Clusters': np.nan,\n",
    "            'Noise_Count': np.nan,\n",
    "            'Noise_Pct': np.nan,\n",
    "            'PCA_Components': np.nan,\n",
    "            'Variance_Explained': np.nan,\n",
    "            'Is_Valid': False,\n",
    "            'Skip_Reason': f\"Error: {str(e)[:50]}\"\n",
    "        })\n",
    "        continue\n",
    "\n",
    "if not cluster_results:\n",
    "    raise ValueError(\"No valid clusters found. Check your data quality and OPTICS parameters.\")\n",
    "\n",
    "cluster_history = pd.concat(cluster_results, ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 1 COMPLETE: Cluster Quality Summary\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# ============================================================================\n",
    "# CLUSTER QUALITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "df_quality = pd.DataFrame(cluster_quality_log)\n",
    "\n",
    "total_timestamps = len(df_quality)\n",
    "valid_timestamps_count = df_quality['Is_Valid'].sum()\n",
    "invalid_timestamps_count = total_timestamps - valid_timestamps_count\n",
    "\n",
    "print(f\"\\nTimestamp Analysis:\")\n",
    "print(f\"  Total timestamps processed: {total_timestamps}\")\n",
    "print(f\"  Valid clustering windows: {valid_timestamps_count} ({valid_timestamps_count/total_timestamps:.1%})\")\n",
    "print(f\"  Invalid/skipped windows: {invalid_timestamps_count} ({invalid_timestamps_count/total_timestamps:.1%})\")\n",
    "\n",
    "if invalid_timestamps_count > 0:\n",
    "    print(f\"\\nSkip Reasons:\")\n",
    "    skip_summary = df_quality[~df_quality['Is_Valid']]['Skip_Reason'].value_counts()\n",
    "    for reason, count in skip_summary.items():\n",
    "        print(f\"  - {reason}: {count} ({count/invalid_timestamps_count:.1%})\")\n",
    "\n",
    "# Valid cluster statistics\n",
    "valid_quality = df_quality[df_quality['Is_Valid']]\n",
    "if len(valid_quality) > 0:\n",
    "    print(f\"\\nValid Cluster Statistics:\")\n",
    "    print(f\"  Avg clusters per timestamp: {valid_quality['Unique_Clusters'].mean():.1f}\")\n",
    "    print(f\"  Avg noise percentage: {valid_quality['Noise_Pct'].mean():.1%}\")\n",
    "    print(f\"  Avg PCA variance retained: {valid_quality['Variance_Explained'].mean():.1%}\")\n",
    "\n",
    "print(f\"\\nCluster History Generated:\")\n",
    "print(f\"  Total rows: {len(cluster_history)}\")\n",
    "print(f\"  Unique timestamps: {cluster_history['Datetime'].nunique()}\")\n",
    "print(f\"  Date range: {cluster_history['Datetime'].min()} to {cluster_history['Datetime'].max()}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2: Cluster Stability Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 2: Analyzing Cluster Stability\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "pair_co_cluster_freq = {}\n",
    "\n",
    "for ts in cluster_history['Datetime'].unique():\n",
    "    snapshot = cluster_history[cluster_history['Datetime'] == ts]\n",
    "    \n",
    "    for cluster_id in snapshot['Cluster_ID'].unique():\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        \n",
    "        members = snapshot[snapshot['Cluster_ID'] == cluster_id]['Ticker'].tolist()\n",
    "        \n",
    "        for s1, s2 in itertools.combinations(sorted(members), 2):\n",
    "            pair_key = (s1, s2)\n",
    "            \n",
    "            if pair_key not in pair_co_cluster_freq:\n",
    "                pair_co_cluster_freq[pair_key] = 0\n",
    "            pair_co_cluster_freq[pair_key] += 1\n",
    "\n",
    "# Calculate frequencies\n",
    "total_valid_windows = cluster_history['Datetime'].nunique()\n",
    "\n",
    "pair_stability_data = []\n",
    "for pair, count in pair_co_cluster_freq.items():\n",
    "    frequency = count / total_valid_windows\n",
    "    pair_stability_data.append({\n",
    "        'Ticker_1': pair[0],\n",
    "        'Ticker_2': pair[1],\n",
    "        'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "        'Co_Cluster_Count': count,\n",
    "        'Co_Cluster_Frequency': frequency,\n",
    "        'Is_Stable': frequency > 0.25\n",
    "    })\n",
    "\n",
    "df_pair_stability = pd.DataFrame(pair_stability_data).sort_values('Co_Cluster_Frequency', ascending=False)\n",
    "\n",
    "print(f\"\\nPair Clustering Analysis:\")\n",
    "print(f\"  Total unique pairs observed: {len(df_pair_stability)}\")\n",
    "print(f\"  Pairs clustered together >50% of time: {(df_pair_stability['Co_Cluster_Frequency'] > 0.50).sum()}\")\n",
    "print(f\"  Pairs clustered together >30% of time: {(df_pair_stability['Co_Cluster_Frequency'] > 0.30).sum()}\")\n",
    "print(f\"  Pairs clustered together <10% of time: {(df_pair_stability['Co_Cluster_Frequency'] < 0.10).sum()}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TOP 15 MOST FREQUENTLY CO-CLUSTERED PAIRS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(df_pair_stability[['Pair', 'Co_Cluster_Count', 'Co_Cluster_Frequency']].head(15).to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TOP 15 MOST TRANSIENT PAIRS (Rare Co-Clustering)\")\n",
    "print(f\"{'='*80}\")\n",
    "transient_pairs = df_pair_stability[\n",
    "    (df_pair_stability['Co_Cluster_Frequency'] > 0.05) &\n",
    "    (df_pair_stability['Co_Cluster_Frequency'] < 0.20)\n",
    "].sort_values('Co_Cluster_Frequency', ascending=True)\n",
    "print(transient_pairs[['Pair', 'Co_Cluster_Count', 'Co_Cluster_Frequency']].head(15).to_string(index=False))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 3: Temporal Analysis\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PHASE 3: Temporal Analysis - When Do Regimes Shift?\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cluster_history['Date'] = pd.to_datetime(cluster_history['Datetime']).dt.date\n",
    "\n",
    "daily_cluster_stats = cluster_history.groupby('Date').agg({\n",
    "    'Cluster_ID': lambda x: len(set(x)) - (1 if -1 in x.values else 0),\n",
    "    'Ticker': 'count'\n",
    "}).rename(columns={'Cluster_ID': 'Num_Clusters', 'Ticker': 'Total_Obs'})\n",
    "\n",
    "print(f\"\\nDaily Clustering Variability:\")\n",
    "print(f\"  Days with high differentiation (>4 clusters): {(daily_cluster_stats['Num_Clusters'] > 4).sum()}\")\n",
    "print(f\"  Days with low differentiation (â‰¤2 clusters): {(daily_cluster_stats['Num_Clusters'] <= 2).sum()}\")\n",
    "\n",
    "mean_clusters = daily_cluster_stats['Num_Clusters'].mean()\n",
    "std_clusters = daily_cluster_stats['Num_Clusters'].std()\n",
    "regime_shift_days = daily_cluster_stats[\n",
    "    abs(daily_cluster_stats['Num_Clusters'] - mean_clusters) > 1.5 * std_clusters\n",
    "]\n",
    "\n",
    "if len(regime_shift_days) > 0:\n",
    "    print(f\"\\nPotential Regime Shift Days (unusual cluster patterns):\")\n",
    "    print(f\"  {len(regime_shift_days)} days detected\")\n",
    "    print(f\"\\nTop 5 Most Unusual Days:\")\n",
    "    top_shifts = regime_shift_days.nlargest(5, 'Num_Clusters')\n",
    "    for date, row in top_shifts.iterrows():\n",
    "        print(f\"  {date}: {row['Num_Clusters']:.0f} clusters (avg: {mean_clusters:.1f})\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CLUSTERING PHASE COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nData structures created:\")\n",
    "print(f\"  - cluster_history: {len(cluster_history)} rows\")\n",
    "print(f\"  - df_quality: {len(df_quality)} rows\")\n",
    "print(f\"  - df_pair_stability: {len(df_pair_stability)} rows\")\n",
    "print(f\"\\nReady for pair testing phase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b954a92",
   "metadata": {},
   "source": [
    "# Cluster Formation & Duration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ebf0ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:21:56.491677Z",
     "iopub.status.busy": "2026-02-23T22:21:56.491489Z",
     "iopub.status.idle": "2026-02-23T22:21:59.078906Z",
     "shell.execute_reply": "2026-02-23T22:21:59.078387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 2B: CLUSTER FORMATION & DISSOLUTION EVENTS\n",
      "================================================================================\n",
      "Tracking 776 pairs across 1364 timestamps...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formation events detected: 20242\n",
      "Dissolution events detected: 20242\n",
      "Complete cluster episodes (formation + dissolution): 20242\n",
      "\n",
      "================================================================================\n",
      "CLUSTER DURATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Duration Statistics (hours):\n",
      "  Mean:   19.9\n",
      "  Median: 1.0\n",
      "  Min:    1\n",
      "  Max:    1178\n",
      "  Std:    44.2\n",
      "\n",
      "Duration Buckets:\n",
      "  Short-lived (<=10h):  14156 (69.9%)\n",
      "  Medium (10-50h):      3615 (17.9%)\n",
      "  Long-lived (>50h):    2471 (12.2%)\n",
      "\n",
      "================================================================================\n",
      "PAIR CLASSIFICATION\n",
      "================================================================================\n",
      "  transient           : 681 pairs\n",
      "  stable_candidate    : 6 pairs\n",
      "  sporadic            : 11 pairs\n",
      "  unknown             : 78 pairs\n",
      "\n",
      "Top 15 Transient Pairs (target for prediction):\n",
      "     Pair  Formation_Count  Avg_Duration  Co_Cluster_Frequency\n",
      "  STX-WDC               75     22.506667              0.177419\n",
      "NXPI-POWI               62     27.403226              0.178886\n",
      " ADI-POWI               56     22.964286              0.124633\n",
      "NXPI-QRVO               55     28.436364              0.140762\n",
      " POWI-STM               54     24.518519              0.134897\n",
      "ASML-LRCX               53     24.622642              0.144428\n",
      " ASML-TSM               52     29.076923              0.151026\n",
      "  LRCX-MU               51     15.254902              0.101906\n",
      "CAMT-NVMI               51     12.980392              0.093842\n",
      "  LSCC-ON               50     24.480000              0.129765\n",
      "  GFS-STM               49     24.836735              0.115836\n",
      "LSCC-MPWR               48     23.583333              0.107038\n",
      "   ADI-ON               48     25.812500              0.118035\n",
      "  MPWR-ON               47     25.744681              0.119501\n",
      "  GFS-TSM               46     23.086957              0.102639\n",
      "\n",
      "Stable/Cointegration Candidates (consistently co-clustered):\n",
      "     Pair  Formation_Count  Avg_Duration  Co_Cluster_Frequency\n",
      "QRVO-SWKS               40    117.225000              0.382698\n",
      "KLAC-LRCX               45     85.288889              0.364370\n",
      "AMAT-LRCX               45     85.044444              0.361437\n",
      "AMAT-KLAC               47     70.127660              0.326246\n",
      " ADI-NXPI               38     93.763158              0.261730\n",
      "  ADI-TXN               52     61.057692              0.256598\n",
      "\n",
      "================================================================================\n",
      "FORMATION/DISSOLUTION ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Data structures created:\n",
      "  - df_formations: 20242 formation events\n",
      "  - df_dissolutions: 20242 dissolution events\n",
      "  - df_durations: 20242 complete cluster episodes\n",
      "  - pair_classification: 776 pairs classified\n",
      "\n",
      "Use df_formations for validation (these are the events to test)\n",
      "Use transient pairs for prediction modeling\n",
      "\n",
      "================================================================================\n",
      "ACTIONABLE FORMATION EVENTS (FIX 5 + FIX 13)\n",
      "================================================================================\n",
      "  All formation events:        20242\n",
      "  With duration metadata:      20242\n",
      "  NOTE: Duration_Hours kept as metadata only (no look-ahead filter)\n",
      "  With duration >= 5h:        6547\n",
      "  With duration >= 10h:        6086\n",
      "  With duration >= 20h:        5845\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CLUSTER FORMATION & DISSOLUTION EVENT DETECTION\n",
    "# ============================================================================\n",
    "# Instead of just counting co-clustering frequency, detect the MOMENTS when\n",
    "# pairs START and STOP co-clustering. These formation events are what we\n",
    "# want to predict.\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"PHASE 2B: CLUSTER FORMATION & DISSOLUTION EVENTS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Build a per-pair, per-timestamp co-clustering indicator\n",
    "# ============================================================================\n",
    "\n",
    "# Get all unique pairs that ever co-clustered (from Phase 2)\n",
    "all_pairs = list(pair_co_cluster_freq.keys())\n",
    "all_timestamps = sorted(cluster_history['Datetime'].unique())\n",
    "\n",
    "print(f\"Tracking {len(all_pairs)} pairs across {len(all_timestamps)} timestamps...\")\n",
    "\n",
    "# Build co-clustering matrix: for each pair, 1 if co-clustered at that timestamp, 0 otherwise\n",
    "pair_coclustering = {}\n",
    "\n",
    "for ts in all_timestamps:\n",
    "    snapshot = cluster_history[cluster_history['Datetime'] == ts]\n",
    "    coclustered_at_ts = set()\n",
    "    \n",
    "    for cluster_id in snapshot['Cluster_ID'].unique():\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "        members = sorted(snapshot[snapshot['Cluster_ID'] == cluster_id]['Ticker'].tolist())\n",
    "        for s1, s2 in itertools.combinations(members, 2):\n",
    "            coclustered_at_ts.add((s1, s2))\n",
    "    \n",
    "    for pair in all_pairs:\n",
    "        if pair not in pair_coclustering:\n",
    "            pair_coclustering[pair] = []\n",
    "        pair_coclustering[pair].append(1 if pair in coclustered_at_ts else 0)\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Detect Formation and Dissolution Events\n",
    "# ============================================================================\n",
    "# A formation event occurs when a pair transitions from NOT co-clustering\n",
    "# to co-clustering (0 -> 1 transition).\n",
    "# A dissolution event is the reverse (1 -> 0 transition).\n",
    "# We require a minimum gap (MIN_GAP_HOURS) of non-co-clustering before a\n",
    "# new formation to avoid counting brief flickers.\n",
    "\n",
    "MIN_GAP_HOURS = 5  # Minimum hours of non-co-clustering before counting a new formation\n",
    "\n",
    "formation_events = []\n",
    "dissolution_events = []\n",
    "cluster_durations = []\n",
    "\n",
    "for pair, series in pair_coclustering.items():\n",
    "    ts_series = list(zip(all_timestamps, series))\n",
    "    \n",
    "    in_cluster = False\n",
    "    formation_ts = None\n",
    "    formation_idx = None\n",
    "    gap_count = MIN_GAP_HOURS  # Start with full gap so first co-clustering counts as formation\n",
    "    \n",
    "    for i, (ts, val) in enumerate(ts_series):\n",
    "        if val == 1:\n",
    "            if not in_cluster and gap_count >= MIN_GAP_HOURS:\n",
    "                # Formation event: pair just started co-clustering after a sufficient gap\n",
    "                in_cluster = True\n",
    "                formation_ts = ts\n",
    "                formation_idx = i\n",
    "                formation_events.append({\n",
    "                    'Ticker_1': pair[0],\n",
    "                    'Ticker_2': pair[1],\n",
    "                    'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "                    'Formation_Time': ts,\n",
    "                    'Timestamp_Index': i\n",
    "                })\n",
    "            elif not in_cluster:\n",
    "                # Still in the gap period, but co-clustering again\n",
    "                in_cluster = True\n",
    "                formation_ts = ts\n",
    "                formation_idx = i\n",
    "            gap_count = 0\n",
    "        else:\n",
    "            if in_cluster:\n",
    "                gap_count += 1\n",
    "                if gap_count >= MIN_GAP_HOURS:\n",
    "                    # Dissolution confirmed after sufficient gap\n",
    "                    # The last co-clustering timestamp was at index (i - gap_count)\n",
    "                    last_cocluster_idx = i - gap_count\n",
    "                    dissolution_ts = all_timestamps[min(last_cocluster_idx + 1, len(all_timestamps) - 1)]\n",
    "                    duration_hours = (all_timestamps[last_cocluster_idx] - all_timestamps[formation_idx]).total_seconds() / 3600.0 if formation_idx is not None else 1.0\n",
    "                    duration = max(1.0, duration_hours)\n",
    "                    \n",
    "                    dissolution_events.append({\n",
    "                        'Ticker_1': pair[0],\n",
    "                        'Ticker_2': pair[1],\n",
    "                        'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "                        'Dissolution_Time': dissolution_ts,\n",
    "                        'Duration_Hours': duration\n",
    "                    })\n",
    "                    \n",
    "                    if formation_ts is not None:\n",
    "                        cluster_durations.append({\n",
    "                            'Ticker_1': pair[0],\n",
    "                            'Ticker_2': pair[1],\n",
    "                            'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "                            'Formation_Time': formation_ts,\n",
    "                            'Dissolution_Time': dissolution_ts,\n",
    "                            'Duration_Hours': duration\n",
    "                        })\n",
    "                    \n",
    "                    in_cluster = False\n",
    "                    formation_ts = None\n",
    "                    formation_idx = None\n",
    "            else:\n",
    "                gap_count += 1\n",
    "\n",
    "    # Handle pairs still in cluster at end of series\n",
    "    if in_cluster and formation_ts is not None:\n",
    "        last_cocluster_idx = len(ts_series) - 1\n",
    "        for j in range(len(ts_series) - 1, -1, -1):\n",
    "            if ts_series[j][1] == 1:\n",
    "                last_cocluster_idx = j\n",
    "                break\n",
    "        duration_hours = (all_timestamps[last_cocluster_idx] - all_timestamps[formation_idx]).total_seconds() / 3600.0 if formation_idx is not None else 1.0\n",
    "        duration = max(1.0, duration_hours)\n",
    "        dissolution_ts = all_timestamps[min(last_cocluster_idx + 1, len(all_timestamps) - 1)]\n",
    "        \n",
    "        dissolution_events.append({\n",
    "            'Ticker_1': pair[0],\n",
    "            'Ticker_2': pair[1],\n",
    "            'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "            'Dissolution_Time': dissolution_ts,\n",
    "            'Duration_Hours': duration\n",
    "        })\n",
    "        \n",
    "        cluster_durations.append({\n",
    "            'Ticker_1': pair[0],\n",
    "            'Ticker_2': pair[1],\n",
    "            'Pair': f\"{pair[0]}-{pair[1]}\",\n",
    "            'Formation_Time': formation_ts,\n",
    "            'Dissolution_Time': dissolution_ts,\n",
    "            'Duration_Hours': duration\n",
    "        })\n",
    "\n",
    "df_formations = pd.DataFrame(formation_events)\n",
    "df_dissolutions = pd.DataFrame(dissolution_events)\n",
    "df_durations = pd.DataFrame(cluster_durations)\n",
    "\n",
    "print(f\"\\nFormation events detected: {len(df_formations)}\")\n",
    "print(f\"Dissolution events detected: {len(df_dissolutions)}\")\n",
    "print(f\"Complete cluster episodes (formation + dissolution): {len(df_durations)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Duration Analysis\n",
    "# ============================================================================\n",
    "\n",
    "if len(df_durations) > 0:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"CLUSTER DURATION ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nDuration Statistics (hours):\")\n",
    "    print(f\"  Mean:   {df_durations['Duration_Hours'].mean():.1f}\")\n",
    "    print(f\"  Median: {df_durations['Duration_Hours'].median():.1f}\")\n",
    "    print(f\"  Min:    {df_durations['Duration_Hours'].min():.0f}\")\n",
    "    print(f\"  Max:    {df_durations['Duration_Hours'].max():.0f}\")\n",
    "    print(f\"  Std:    {df_durations['Duration_Hours'].std():.1f}\")\n",
    "    \n",
    "    # Duration buckets\n",
    "    short_lived = len(df_durations[df_durations['Duration_Hours'] <= 10])\n",
    "    medium_lived = len(df_durations[(df_durations['Duration_Hours'] > 10) & (df_durations['Duration_Hours'] <= 50)])\n",
    "    long_lived = len(df_durations[df_durations['Duration_Hours'] > 50])\n",
    "    \n",
    "    print(f\"\\nDuration Buckets:\")\n",
    "    print(f\"  Short-lived (<=10h):  {short_lived} ({short_lived/len(df_durations):.1%})\")\n",
    "    print(f\"  Medium (10-50h):      {medium_lived} ({medium_lived/len(df_durations):.1%})\")\n",
    "    print(f\"  Long-lived (>50h):    {long_lived} ({long_lived/len(df_durations):.1%})\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Classify pairs\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PAIR CLASSIFICATION\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Transient pairs: multiple formation events with short durations\n",
    "pair_formation_counts = df_formations.groupby('Pair').size().reset_index(name='Formation_Count')\n",
    "pair_avg_duration = df_durations.groupby('Pair')['Duration_Hours'].mean().reset_index(name='Avg_Duration')\n",
    "\n",
    "pair_classification = pair_formation_counts.merge(pair_avg_duration, on='Pair', how='left')\n",
    "pair_classification = pair_classification.merge(\n",
    "    df_pair_stability[['Pair', 'Co_Cluster_Frequency']], on='Pair', how='left'\n",
    ")\n",
    "\n",
    "# Classify\n",
    "pair_classification['Category'] = 'unknown'\n",
    "\n",
    "# Transient: multiple short-lived formations (the target use case)\n",
    "transient_mask = (pair_classification['Formation_Count'] >= 3) & (pair_classification['Avg_Duration'] <= 30)\n",
    "pair_classification.loc[transient_mask, 'Category'] = 'transient'\n",
    "\n",
    "# Stable: high co-clustering frequency OR very long average duration\n",
    "stable_mask = ((pair_classification['Co_Cluster_Frequency'] > 0.25) | (pair_classification['Avg_Duration'] > 100)) & (pair_classification['Category'] == 'unknown')\n",
    "pair_classification.loc[stable_mask, 'Category'] = 'stable_candidate'\n",
    "\n",
    "# Sporadic: few formation events\n",
    "sporadic_mask = (pair_classification['Formation_Count'] <= 2) & (pair_classification['Category'] == 'unknown')\n",
    "pair_classification.loc[sporadic_mask, 'Category'] = 'sporadic'\n",
    "\n",
    "for cat in ['transient', 'stable_candidate', 'sporadic', 'unknown']:\n",
    "    count = len(pair_classification[pair_classification['Category'] == cat])\n",
    "    print(f\"  {cat:20s}: {count} pairs\")\n",
    "\n",
    "# Show top transient pairs\n",
    "transient_pairs_classified = pair_classification[pair_classification['Category'] == 'transient'].sort_values('Formation_Count', ascending=False)\n",
    "if len(transient_pairs_classified) > 0:\n",
    "    print(f\"\\nTop 15 Transient Pairs (target for prediction):\")\n",
    "    print(transient_pairs_classified[['Pair', 'Formation_Count', 'Avg_Duration', 'Co_Cluster_Frequency']].head(15).to_string(index=False))\n",
    "\n",
    "# Show stable/cointegration candidates\n",
    "stable_candidates = pair_classification[pair_classification['Category'] == 'stable_candidate'].sort_values('Co_Cluster_Frequency', ascending=False)\n",
    "if len(stable_candidates) > 0:\n",
    "    print(f\"\\nStable/Cointegration Candidates (consistently co-clustered):\")\n",
    "    print(stable_candidates[['Pair', 'Formation_Count', 'Avg_Duration', 'Co_Cluster_Frequency']].head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FORMATION/DISSOLUTION ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nData structures created:\")\n",
    "print(f\"  - df_formations: {len(df_formations)} formation events\")\n",
    "print(f\"  - df_dissolutions: {len(df_dissolutions)} dissolution events\")\n",
    "print(f\"  - df_durations: {len(df_durations)} complete cluster episodes\")\n",
    "print(f\"  - pair_classification: {len(pair_classification)} pairs classified\")\n",
    "print(f\"\\nUse df_formations for validation (these are the events to test)\")\n",
    "print(f\"Use transient pairs for prediction modeling\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIX 5: Attach duration metadata (without filtering by future-known duration)\n",
    "# ============================================================================\n",
    "# Duration_Hours is kept as metadata for analysis but NOT used to filter,\n",
    "# because duration is only known after the episode ends (look-ahead bias).\n",
    "\n",
    "MIN_EPISODE_HOURS = 5\n",
    "\n",
    "if len(df_durations) > 0:\n",
    "    df_formations_actionable = df_formations.merge(\n",
    "        df_durations[['Pair', 'Formation_Time', 'Duration_Hours']],\n",
    "        on=['Pair', 'Formation_Time'], how='left'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"ACTIONABLE FORMATION EVENTS (FIX 5 + FIX 13)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"  All formation events:        {len(df_formations)}\")\n",
    "    print(f\"  With duration metadata:      {df_formations_actionable['Duration_Hours'].notna().sum()}\")\n",
    "    print(f\"  NOTE: Duration_Hours kept as metadata only (no look-ahead filter)\")\n",
    "    \n",
    "    for bucket_h in [5, 10, 20]:\n",
    "        n = len(df_formations_actionable[df_formations_actionable['Duration_Hours'] >= bucket_h])\n",
    "        print(f\"  With duration >= {bucket_h}h:        {n}\")\n",
    "else:\n",
    "    df_formations_actionable = df_formations.copy()\n",
    "    print(\"WARNING: No duration data; using all formations as actionable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c219101",
   "metadata": {},
   "source": [
    "# Correctness Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45a35d19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:21:59.081213Z",
     "iopub.status.busy": "2026-02-23T22:21:59.081075Z",
     "iopub.status.idle": "2026-02-23T22:23:26.534590Z",
     "shell.execute_reply": "2026-02-23T22:23:26.527640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CORRECTNESS CHECK 1: Feature-Shuffle Permutation Test (FIX 1)\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fraction of pairs significant at p<0.05: 13.5%\n",
      "\n",
      "Top 15 pairs by permutation Z-score:\n",
      "  KLAC  -LRCX    Z = 16.40\n",
      "  AMAT  -KLAC    Z = 15.61\n",
      "  QRVO  -SWKS    Z = 14.18\n",
      "  AMAT  -LRCX    Z = 12.70\n",
      "  MCHP  -POWI    Z = 8.97\n",
      "  KLAC  -TSM     Z = 8.84\n",
      "  ADI   -SWKS    Z = 7.84\n",
      "  POWI  -SWKS    Z = 7.47\n",
      "  ASML  -LRCX    Z = 7.37\n",
      "  AMAT  -ASML    Z = 6.98\n",
      "  NXPI  -STM     Z = 6.96\n",
      "  ADI   -NXPI    Z = 6.85\n",
      "  QCOM  -SWKS    Z = 6.83\n",
      "  MCHP  -ON      Z = 6.65\n",
      "  NXPI  -POWI    Z = 6.47\n",
      "\n",
      "================================================================================\n",
      "CORRECTNESS CHECK 2: Out-of-Sample Split\n",
      "================================================================================\n",
      "\n",
      "Split point: 2025-10-31 13:30:00+00:00\n",
      "Training period: 2025-03-24 16:30:00+00:00 to 2025-10-31 13:30:00+00:00 (36551 rows)\n",
      "Test period: 2025-10-31 13:30:00+00:00 to 2026-02-23 20:30:00+00:00 (17996 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairs observed in both periods: 747\n",
      "Correlation of co-clustering frequency (train vs test): 0.665 (p=0.0000)\n",
      "RESULT: Good out-of-sample stability\n",
      "\n",
      "================================================================================\n",
      "CORRECTNESS CHECK 3: OPTICS Parameter Sensitivity\n",
      "================================================================================\n",
      "Testing 5 configs on 50 timestamps...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Sensitivity Results:\n",
      " min_samples   xi  min_cluster_size  avg_clusters  avg_noise_pct  std_clusters\n",
      "           2 0.05                 2          8.44         0.4035      1.733897\n",
      "           3 0.05                 3          3.30         0.5970      1.187434\n",
      "           3 0.03                 3          3.60         0.5405      1.077033\n",
      "           5 0.05                 5          1.30         0.6570      0.500000\n",
      "           3 0.10                 3          2.50         0.6840      0.964365\n",
      "\n",
      "================================================================================\n",
      "ALL CORRECTNESS CHECKS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CORRECTNESS CHECKS: Feature-Shuffle Permutation Test, OOS Split, Sensitivity\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# 1. FEATURE-SHUFFLE PERMUTATION TEST (FIX 1)\n",
    "# ============================================================================\n",
    "# The old test shuffled cluster *labels*, which preserves cluster sizes and\n",
    "# therefore preserves co-clustering rates by construction (tautological).\n",
    "#\n",
    "# The correct approach: shuffle the *feature vectors* across tickers at each\n",
    "# timestamp, then re-run the full StandardScaler -> PCA -> OPTICS pipeline.\n",
    "# This breaks the ticker-feature mapping while preserving the joint\n",
    "# distribution of features, providing a proper null hypothesis.\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"CORRECTNESS CHECK 1: Feature-Shuffle Permutation Test (FIX 1)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "optics_params = dict(min_samples=3, metric='euclidean', xi=0.05, min_cluster_size=3)\n",
    "features_to_cluster = [\n",
    "    'Returns', 'Vol_Short', 'Beta_SPX_Short', 'Beta_Sector_Short',\n",
    "    'RSI', 'Momentum_5H', 'Vol_Regime_Shift', 'Beta_SPX_Regime_Shift',\n",
    "    'Beta_Sector_Regime_Shift'\n",
    "]\n",
    "\n",
    "perm_result = feature_shuffle_permutation_test(\n",
    "    ts_df=ts_df,\n",
    "    features_to_cluster=features_to_cluster,\n",
    "    optics_params=optics_params,\n",
    "    pair_co_cluster_freq=pair_co_cluster_freq,\n",
    "    total_valid_windows=cluster_history['Datetime'].nunique(),\n",
    "    n_permutations=30,\n",
    "    n_sample_timestamps=80,\n",
    ")\n",
    "\n",
    "print(f\"\\nFraction of pairs significant at p<0.05: {perm_result['fraction_significant']:.1%}\")\n",
    "\n",
    "# Show top pairs by Z-score\n",
    "top_z = sorted(perm_result['pair_zscores'].items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "print(f\"\\nTop 15 pairs by permutation Z-score:\")\n",
    "for pair, z in top_z:\n",
    "    print(f\"  {pair[0]:6s}-{pair[1]:6s}  Z = {z:.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. OUT-OF-SAMPLE SPLIT\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CORRECTNESS CHECK 2: Out-of-Sample Split\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "all_dates = sorted(cluster_history['Datetime'].unique())\n",
    "split_point = int(len(all_dates) * 0.67)\n",
    "split_timestamp = all_dates[split_point]\n",
    "\n",
    "train_history = cluster_history[cluster_history['Datetime'] <= split_timestamp]\n",
    "test_history = cluster_history[cluster_history['Datetime'] > split_timestamp]\n",
    "\n",
    "print(f\"\\nSplit point: {split_timestamp}\")\n",
    "print(f\"Training period: {all_dates[0]} to {split_timestamp} ({len(train_history)} rows)\")\n",
    "print(f\"Test period: {split_timestamp} to {all_dates[-1]} ({len(test_history)} rows)\")\n",
    "\n",
    "def calc_pair_freq(history_df):\n",
    "    freq = {}\n",
    "    total = history_df['Datetime'].nunique()\n",
    "    for ts in history_df['Datetime'].unique():\n",
    "        snap = history_df[history_df['Datetime'] == ts]\n",
    "        for cid in snap['Cluster_ID'].unique():\n",
    "            if cid == -1:\n",
    "                continue\n",
    "            members = sorted(snap[snap['Cluster_ID'] == cid]['Ticker'].tolist())\n",
    "            for s1, s2 in itertools.combinations(members, 2):\n",
    "                freq[(s1, s2)] = freq.get((s1, s2), 0) + 1\n",
    "    return {k: v / total for k, v in freq.items()}\n",
    "\n",
    "train_freq = calc_pair_freq(train_history)\n",
    "test_freq = calc_pair_freq(test_history)\n",
    "\n",
    "common_pairs = set(train_freq.keys()) & set(test_freq.keys())\n",
    "if len(common_pairs) > 0:\n",
    "    train_vals = [train_freq[p] for p in common_pairs]\n",
    "    test_vals = [test_freq[p] for p in common_pairs]\n",
    "    from scipy.stats import pearsonr\n",
    "    oos_corr, oos_pval = pearsonr(train_vals, test_vals)\n",
    "    \n",
    "    print(f\"\\nPairs observed in both periods: {len(common_pairs)}\")\n",
    "    print(f\"Correlation of co-clustering frequency (train vs test): {oos_corr:.3f} (p={oos_pval:.4f})\")\n",
    "    \n",
    "    if oos_corr > 0.5:\n",
    "        print(\"RESULT: Good out-of-sample stability\")\n",
    "    elif oos_corr > 0.2:\n",
    "        print(\"RESULT: Moderate out-of-sample stability\")\n",
    "    else:\n",
    "        print(\"WARNING: Poor out-of-sample stability\")\n",
    "else:\n",
    "    print(\"WARNING: No common pairs between train and test periods\")\n",
    "\n",
    "oos_split_timestamp = split_timestamp\n",
    "\n",
    "# ============================================================================\n",
    "# 3. OPTICS PARAMETER SENSITIVITY CHECK\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CORRECTNESS CHECK 3: OPTICS Parameter Sensitivity\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "param_configs = [\n",
    "    {'min_samples': 2, 'xi': 0.05, 'min_cluster_size': 2},\n",
    "    {'min_samples': 3, 'xi': 0.05, 'min_cluster_size': 3},\n",
    "    {'min_samples': 3, 'xi': 0.03, 'min_cluster_size': 3},\n",
    "    {'min_samples': 5, 'xi': 0.05, 'min_cluster_size': 5},\n",
    "    {'min_samples': 3, 'xi': 0.10, 'min_cluster_size': 3},\n",
    "]\n",
    "\n",
    "sample_timestamps_sens = valid_timestamps[::max(1, len(valid_timestamps) // 50)][:50]\n",
    "print(f\"Testing {len(param_configs)} configs on {len(sample_timestamps_sens)} timestamps...\\n\")\n",
    "\n",
    "sensitivity_results = []\n",
    "for config in param_configs:\n",
    "    config_clusters = []\n",
    "    config_noise = []\n",
    "    for ts in sample_timestamps_sens:\n",
    "        try:\n",
    "            snapshot = ts_df.xs(ts, level='Datetime')[features_to_cluster].dropna()\n",
    "            if len(snapshot) < 5:\n",
    "                continue\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(snapshot)\n",
    "            pca = PCA(n_components=0.90)\n",
    "            X_pca = pca.fit_transform(X_scaled)\n",
    "            optics = OPTICS(min_samples=config['min_samples'], metric='euclidean',\n",
    "                            xi=config['xi'], min_cluster_size=config['min_cluster_size'])\n",
    "            optics.fit(X_pca)\n",
    "            n_clusters = len(set(optics.labels_)) - (1 if -1 in optics.labels_ else 0)\n",
    "            noise_pct = (optics.labels_ == -1).sum() / len(optics.labels_)\n",
    "            config_clusters.append(n_clusters)\n",
    "            config_noise.append(noise_pct)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if config_clusters:\n",
    "        sensitivity_results.append({\n",
    "            'min_samples': config['min_samples'],\n",
    "            'xi': config['xi'],\n",
    "            'min_cluster_size': config['min_cluster_size'],\n",
    "            'avg_clusters': np.mean(config_clusters),\n",
    "            'avg_noise_pct': np.mean(config_noise),\n",
    "            'std_clusters': np.std(config_clusters)\n",
    "        })\n",
    "\n",
    "df_sensitivity = pd.DataFrame(sensitivity_results)\n",
    "print(\"Parameter Sensitivity Results:\")\n",
    "print(df_sensitivity.to_string(index=False))\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL CORRECTNESS CHECKS COMPLETE\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00045a",
   "metadata": {},
   "source": [
    "# Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "276ee294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T22:23:26.547375Z",
     "iopub.status.busy": "2026-02-23T22:23:26.546988Z",
     "iopub.status.idle": "2026-02-23T22:23:26.725400Z",
     "shell.execute_reply": "2026-02-23T22:23:26.724198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ts_df -> /Users/jack/Documents/code/pairs-test/research/data/ts_df.pkl (63143 items)\n",
      "Saved df_formations -> /Users/jack/Documents/code/pairs-test/research/data/df_formations.pkl (20242 items)\n",
      "Saved df_formations_actionable -> /Users/jack/Documents/code/pairs-test/research/data/df_formations_actionable.pkl (20242 items)\n",
      "Saved pair_classification -> /Users/jack/Documents/code/pairs-test/research/data/pair_classification.pkl (776 items)\n",
      "Saved cluster_history -> /Users/jack/Documents/code/pairs-test/research/data/cluster_history.pkl (54547 items)\n",
      "Saved df_durations -> /Users/jack/Documents/code/pairs-test/research/data/df_durations.pkl (20242 items)\n",
      "Saved df_pair_stability -> /Users/jack/Documents/code/pairs-test/research/data/df_pair_stability.pkl (776 items)\n",
      "Saved oos_split_timestamp -> /Users/jack/Documents/code/pairs-test/research/data/oos_split_timestamp.pkl (1 items)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pair_co_cluster_freq -> /Users/jack/Documents/code/pairs-test/research/data/pair_co_cluster_freq.pkl (776 items)\n",
      "\n",
      "All artifacts saved to /Users/jack/Documents/code/pairs-test/research/data/\n",
      "optics-signals.ipynb can now load these files.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SAVE ARTIFACTS FOR SIGNALS NOTEBOOK\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "data_dir = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "save_items = {\n",
    "    'ts_df': ts_df,\n",
    "    'df_formations': df_formations,\n",
    "    'df_formations_actionable': df_formations_actionable,\n",
    "    'pair_classification': pair_classification,\n",
    "    'cluster_history': cluster_history,\n",
    "    'df_durations': df_durations,\n",
    "    'df_pair_stability': df_pair_stability,\n",
    "    'oos_split_timestamp': oos_split_timestamp,\n",
    "    'pair_co_cluster_freq': pair_co_cluster_freq,\n",
    "}\n",
    "\n",
    "for name, obj in save_items.items():\n",
    "    path = os.path.join(data_dir, f'{name}.pkl')\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    size = len(obj) if hasattr(obj, '__len__') else 1\n",
    "    print(f'Saved {name} -> {path} ({size} items)')\n",
    "\n",
    "print(f'\\nAll artifacts saved to {data_dir}/')\n",
    "print('optics-signals.ipynb can now load these files.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
